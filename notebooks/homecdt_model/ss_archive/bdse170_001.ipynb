{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row, DataFrame, Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.18.1', '1.0.1')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__, pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"yarn\")\\\n",
    "    .config('spark.executor.cores','2') \\\n",
    "    .config('spark.executor.instances','3') \\\n",
    "    .config(\"spark.executor.memory\", '8g')\\\n",
    "    .appName(\"ss_001\")\\\n",
    "    .getOrCreate()\n",
    "# .config(\"spark.jars.packages\", \"com.microsoft.ml.spark:mmlspark_2.11:1.0.0-rc1\")\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://bdse170.example.org:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ss</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb275796d68>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Parquet files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do only once per datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read.csv('/user/ss/datasets/ss_fteng_G3V3_ohe_recip_20200218a.csv', \\\n",
    "#                     sep = ',', header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DataFrames can be saved as Parquet files, maintaining the schema information.\n",
    "# df.write.parquet('/user/ss/datasets/ss_fteng_G3V3_ohe_recip_20200218a.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading files from parquet\n",
    "df = spark.read.parquet('/user/ss/datasets/ss_fteng_G3V3_ohe_recip_20200218a.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df = final_df.repartition(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df['TARGET'].isNotNull()]\n",
    "test_df = df[df['TARGET'].isNull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert df.count() == (train_df.count() + test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # spark機器學習要求輸入的DataFrame類型為數值類型， 將本來的string欄位轉換成double，並替代空值\n",
    "# for col, t in app_train.dtypes:\n",
    "#     if t == \"string\":\n",
    "#         app_train = app_train.withColumn(col, app_train[col].cast(\"double\"))\n",
    "\n",
    "# app_train = app_train.withColumn(\"TARGET\", app_train[\"TARGET\"].cast(\"int\"))\n",
    "# app_train = app_train.fillna(999999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 跟在普通單機上做訓練時不同，spark做訓練時所有特徵列需要通過VectorAssembler轉換成特徵矩陣，才能用來訓練\n",
    "import pyspark.ml.feature as ft\n",
    "featuresCreator = ft.VectorAssembler(\n",
    "    inputCols=[col for col in train_df.columns[:] if col not in [\"TARGET\"]],\n",
    "    outputCol='features'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### info funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # function\n",
    "# def info(slef):\n",
    "#     for i in spark.sparkContext._conf.getAll():\n",
    "#         if i[0] in ['spark.executor.instances','spark.executor.cores','spark.executor.memory']:\n",
    "#             print(f'{i[0]} : {i[1]}')\n",
    "#     print(f'Partitions : {app_train.rdd.getNumPartitions()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 實例化一個LightGBM Regressor， 其參數和單機版本類似但不盡相同， 文檔可以在以下鏈接找到：\n",
    "# https://mmlspark.azureedge.net/docs/pyspark/LightGBMRegressor.html\n",
    "from mmlspark.lightgbm import LightGBMRegressor\n",
    "\n",
    "lgbm = LightGBMRegressor(\n",
    "    boostingType=\"goss\",\n",
    "    numIterations=100,\n",
    "    objective='binary',\n",
    "    learningRate=0.1,\n",
    "#     baggingSeed=50,\n",
    "    lambdaL1=0.8,\n",
    "    lambdaL2=0.8,\n",
    "#     baggingFraction=0.87,\n",
    "    minSumHessianInLeaf=0.03,\n",
    "    maxDepth=12,\n",
    "#     featureFraction=0.66,\n",
    "    numLeaves=31,\n",
    "    labelCol=\"TARGET\"\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立一個pipeline，簡化訓練步驟\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "                # 特徵整理\n",
    "                featuresCreator,\n",
    "                # 模型名稱\n",
    "                    lgbm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 這裡是將訓練數據分成訓練集和驗證集，測試模型預測效果 (OK)\n",
    "# import pyspark.ml.evaluation as ev\n",
    "# from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# tr, tv = train_df.randomSplit([0.7, 0.3], seed = 924)\n",
    "\n",
    "# vmodel = pipeline.fit(tr)\n",
    "# t_prediction = vmodel.transform(tv)\n",
    "# evaluator = ev.BinaryClassificationEvaluator(\n",
    "#      rawPredictionCol='prediction',\n",
    "#      labelCol='TARGET')\n",
    "# print(evaluator.evaluate(t_prediction, {evaluator.metricName: 'areaUnderROC'}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交叉驗證\n",
    "import pyspark.ml.evaluation as ev\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lgbm.learningRate, [0.1])\\\n",
    "    .build()\n",
    "\n",
    "\n",
    "evaluator = ev.RegressionEvaluator(\n",
    "     labelCol='TARGET',\n",
    "     rawPredictionCol='prediction')\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator = pipeline,\n",
    "    estimatorParamMaps = paramGrid,\n",
    "    evaluator = evaluator,\n",
    "    numFolds = 2)\n",
    "\n",
    "# cv_pipeline = Pipeline(stages=[featuresCreator,cv])\n",
    "\n",
    "cv_model = cv.fit(train_df)\n",
    "cv_prediction = cv_model.transform(test_df)\n",
    "\n",
    "print(f'Full AUC {evaluator.evaluate(cv_prediction)}')\n",
    "print(evaluator.evaluate(cv_model, {evaluator.metricName: 'areaUnderROC'}))\n",
    "\n",
    "selected = cv_prediction.select(\"SK_ID_CURR\", \"TARGET\")\n",
    "for row in selected.collect():\n",
    "    print(row)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = cv_prediction.select(\"SK_ID_CURR\", \"TARGET\")\n",
    "for row in selected.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 找出最佳模型\n",
    "# from pyspark.ml.tuning import ParamGridBuilder\n",
    "# from pyspark.ml.tuning import TrainValidationSplit\n",
    "\n",
    "# paramGrid = ParamGridBuilder()\\\n",
    "#     .addGrid(lgbm.numLeaves, [10,20,30])\\\n",
    "#     .addGrid(lgbm.numIterations, [100,160,200])\\\n",
    "#     .addGrid(lgbm.baggingSeed, [25,50,75])\\\n",
    "#     .build()\n",
    "# tvs = TrainValidationSplit( estimator=lgbm,\n",
    "#     estimatorParamMaps=paramGrid,\n",
    "#     evaluator=evaluator,\n",
    "#     trainRatio = 0.8)\n",
    "# # 最佳模型\n",
    "# tvs_pipeline = Pipeline(stages=[featuresCreator,tvs])\n",
    "\n",
    "# tvs_pipelineModel = tvs_pipeline.fit(train_df)\n",
    "\n",
    "# prediction = tvs_pipelineModel.transform(test_df)\n",
    "# print(f'Full AUC {evaluator.evaluate(prediction)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试集结果输出，从hadoop里将预测数据下载到本机\n",
    "res = prediction.select(\"SK_ID_CURR\", \"prediction\")\n",
    "res = res.withColumn(\"TARGET\", res[\"prediction\"])\n",
    "res = res.select(\"SK_ID_CURR\", \"TARGET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.coalesce(1).write.csv(\"./cluster_lgbm.csv\", header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zhuanlan.zhihu.com/p/67828512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmlspark import LightGBMRegressor\n",
    "import pyspark.ml.feature as ft\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.ml.evaluation as ev\n",
    "import pyspark.sql.types as typ\n",
    "\n",
    "# 如果你是通过spark-submit来运行，则需要先实例化一个spark session对象， 在pyspark中spark session对象已经默认生成\n",
    "# from pyspark import SparkConf, SparkContext\n",
    "# from pyspark.sql import SparkSession\n",
    "# conf = SparkConf().setMaster(\"spark://master:7077\").setAppName(\"MMLSPARK\")\n",
    "# sc = SparkContext(conf = conf)\n",
    "# spark = SparkSession \\\n",
    "#         .builder \\\n",
    "#         .appName(\"MMLSPARK\") \\\n",
    "#         .enableHiveSupport() \\\n",
    "#         .getOrCreate()\n",
    "\n",
    "\n",
    "# 读取csv数据，这里读取的是事先使用hadoop fs -put命令上传到hadoop里的数据\n",
    "app_train = spark.read.csv(\"/homecredit/train_all3.csv\", header='true', inferSchema='true')\n",
    "\n",
    "# 数据预处理， 将本来应该是数字的字符串数据转化数据类型，并替代空值\n",
    "for col, t in app_train.dtypes:\n",
    "    if t == \"string\":\n",
    "        app_train = app_train.withColumn(col, app_train[col].cast(\"double\"))\n",
    "\n",
    "app_train = app_train.withColumn(\"TARGET\", app_train[\"TARGET\"].cast(\"int\"))\n",
    "app_train = app_train.fillna(999999)\n",
    "\n",
    "# 跟在普通单机上做训练时不同，spark做训练时所有特征列需要通过VectorAssembler转换成特征矩阵，才能用来训练\n",
    "featuresCreator = ft.VectorAssembler(\n",
    "    inputCols=[col for col in app_train.columns[1:] if col != \"TARGET\"],\n",
    "    outputCol='features'\n",
    "    )\n",
    "\n",
    "\n",
    "# 实例化一个LightGBM Regressor， 其参数和单机版本类似但不尽相同， 文档可以在以下链接找到：\n",
    "# https://mmlspark.azureedge.net/docs/pyspark/LightGBMRegressor.html\n",
    "lgbm = LightGBMRegressor(numIterations=120, objective='binary',\n",
    "        learningRate=0.007, baggingSeed=50,\n",
    "        boostingType=\"goss\", lambdaL1=0.4, lambdaL2=0.4,\n",
    "        baggingFraction=0.87, minSumHessianInLeaf=0.003,\n",
    "        maxDepth=9, featureFraction=0.66, numLeaves=47,\n",
    "        labelCol=\"TARGET\"\n",
    "                          )\n",
    "\n",
    "# 建立一个pipeline，简化训练步骤\n",
    "pipeline = Pipeline(stages=[\n",
    "                # 特征整理\n",
    "                featuresCreator,\n",
    "                # 模型名称\n",
    "                    lgbm])\n",
    "\n",
    "# 这里是将数据分成训练集和验证集，测试模型预测效果\n",
    "tr, te = app_train.randomSplit([0.7, 0.3], seed=666)\n",
    "\n",
    "vmodel = pipeline.fit(tr)\n",
    "t_model = vmodel.transform(te)\n",
    "evaluator = ev.BinaryClassificationEvaluator(\n",
    "     rawPredictionCol='prediction',\n",
    "     labelCol='TARGET')\n",
    "print(evaluator.evaluate(t_model,\n",
    " {evaluator.metricName: 'areaUnderROC'}))\n",
    "\n",
    "# 实际训练过程\n",
    "model = pipeline.fit(app_train)\n",
    "\n",
    "# 测试集的数据预处理和训练\n",
    "app_test = spark.read.csv(\"/homecredit/test_all3.csv\", header='true', inferSchema='true')\n",
    "for col, t in app_test.dtypes:\n",
    "    if t == \"string\":\n",
    "        app_test = app_test.withColumn(col, app_test[col].cast(\"double\"))\n",
    "app_test = app_test.fillna(999999)\n",
    "prediction = model.transform(app_test)\n",
    "\n",
    "# 测试集结果输出，从hadoop里将预测数据下载到本机\n",
    "res = prediction.select(\"SK_ID_CURR\", \"prediction\")\n",
    "res = res.withColumn(\"TARGET\", res[\"prediction\"])\n",
    "res = res.select(\"SK_ID_CURR\", \"TARGET\")\n",
    "res.coalesce(1).write.csv(\"/homecredit/cluster_lgbm.csv\", header='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 交叉驗證\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lgbm.learningRate, [0.095,0.1,0.105])\\\n",
    "    .build()\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=lgbm,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds = 5)\n",
    "\n",
    "cv_pipeline = Pipeline(stages=[featuresCreator,cv])\n",
    "\n",
    "cv_model = cv_pipeline.fit(train_df)\n",
    "cv_prediction = cv_model.transform(test_df)\n",
    "\n",
    "print(f'Full AUC {evaluator.evaluate(cv_prediction)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 找出最佳模型\n",
    "\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lgbm.numLeaves, [10,20,30])\\\n",
    "    .addGrid(lgbm.numIterations, [100,160,200])\\\n",
    "    .addGrid(lgbm.baggingSeed, [25,50,75])\\\n",
    "    .build()\n",
    "tvs = TrainValidationSplit( estimator=lgbm,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    trainRatio = 0.8)\n",
    "# 最佳模型\n",
    "tvs_pipeline = Pipeline(stages=[featuresCreator,tvs])\n",
    "\n",
    "tvs_pipelineModel = tvs_pipeline.fit(train_df)\n",
    "\n",
    "prediction = tvs_pipelineModel.transform(test_df)\n",
    "print(f'Full AUC {evaluator.evaluate(prediction)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 找出最佳模型+交叉驗證\n",
    "\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lgbm.numLeaves, [10,20,30])\\\n",
    "    .addGrid(lgbm.numIterations, [100,200,300])\\\n",
    "    .addGrid(lgbm.baggingSeed, [25,50,75])\\\n",
    "    .build()\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=lgbm,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds = 10)\n",
    "\n",
    "tvs_pipeline = Pipeline(stages=[featuresCreator,cv])\n",
    "\n",
    "tvs_pipelineModel = tvs_pipeline.fit(train_df)\n",
    "\n",
    "prediction = tvs_pipelineModel.transform(test_df)\n",
    "print(f'Full AUC {evaluator.evaluate(prediction)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/latest/ml-tuning.html\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Prepare training documents, which are labeled.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0),\n",
    "    (4, \"b spark who\", 1.0),\n",
    "    (5, \"g d a y\", 0.0),\n",
    "    (6, \"spark fly\", 1.0),\n",
    "    (7, \"was mapreduce\", 0.0),\n",
    "    (8, \"e spark program\", 1.0),\n",
    "    (9, \"a e c l\", 0.0),\n",
    "    (10, \"spark compile\", 1.0),\n",
    "    (11, \"hadoop software\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "# This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n",
    "# this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=2)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(training)\n",
    "\n",
    "# Prepare test documents, which are unlabeled.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"mapreduce spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = cvModel.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
