{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is based on 'bear_Final_model' released 2020/01/23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forked from excellent kernel : https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features\n",
    "# From Kaggler : https://www.kaggle.com/jsaguiar\n",
    "# Just added a few features so I thought I had to make release it as well...\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the saved dtypes Series\n",
    "final_df_dtypes = \\\n",
    "pd.read_csv('../../../BDSE12-Group3/datasets/homecdt_ss_output/ss_fteng_fromBDSE12_03G_HomeCredit_V1_20200201b_dtypes_series.csv'\\\n",
    "            , header=None, index_col=0, squeeze=True)\n",
    "del final_df_dtypes.index.name\n",
    "final_df_dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the newly read csv with correct dtypes\n",
    "# It seemed not necessary to transform it to dict: final_df_dtype_dict = final_df_dtype_dict.to_dict()\n",
    "# final_df = final_df.astype(final_df_dtypes)\n",
    "# # Or when reading the dataset csv\n",
    "# final_df = pd.read_csv('filepath', dtype = final_df_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = \\\n",
    "pd.read_csv('../../../BDSE12-Group3/datasets/homecdt_ss_output/ss_fteng_fromBDSE12_03G_HomeCredit_V1_20200201b.csv'\\\n",
    "           , dtype= final_df_dtypes)\n",
    "# final_df = pd.read_csv('filepath', dtype = final_df_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int_colName = final_df.columns[final_df.dtypes == (np.int64 or np.int32)]\n",
    "# float_colName = final_df.columns[final_df.dtypes == (np.float64 or np.float32)]\n",
    "# int_df = final_df.loc[:, int_colName]\n",
    "# float_df = final_df.loc[:, float_colName]\n",
    "# int_df.shape, float_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert int_df.shape[1] + float_df.shape[1] == final_df.shape[1]\n",
    "assert all(np.isfinite(int_df).all().value_counts())==True\n",
    "assert all(np.isfinite(float_df).all().value_counts())==True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# int_df.max().max(), int_df.min().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# float_df.max().max(), float_df.min().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do this only once for each dataset\n",
    "def reduce_mem_usage3(props):\n",
    "    props.info(verbose=False)\n",
    "    \n",
    "    int_colName = props.columns[props.dtypes == (np.int64 or np.int32)]\n",
    "    float_colName = props.columns[props.dtypes == (np.float64 or np.float32)]\n",
    "    int_df = props.loc[:, int_colName]\n",
    "    float_df = props.loc[:, float_colName]\n",
    "    print(int_df.shape)\n",
    "    print(float_df.shape)\n",
    "    \n",
    "    assert int_df.shape[1] + float_df.shape[1] == props.shape[1]\n",
    "    assert all(np.isfinite(int_df).all().value_counts())==True\n",
    "    assert all(np.isfinite(float_df).all().value_counts())==True\n",
    "    \n",
    "    print(\"=============================================================\")\n",
    "    start_mem_usg = props.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n",
    "    print(\"=============================================================\")\n",
    "    print(\"Start working on integers...\")\n",
    "    int_df[int_df.columns[(int_df.min()>=0) & (int_df.max()<255)]] = \\\n",
    "    int_df.loc[:, int_df.columns[(int_df.min()>=0) & (int_df.max()<255)]].astype(np.uint8, copy=False)\n",
    "    int_df.info(verbose=False)\n",
    "    print(\"=============================================================\")\n",
    "\n",
    "    int_df[int_df.columns[(int_df.min()>=0) &(int_df.max() >= 255) & (int_df.max()<65535)]] = \\\n",
    "    int_df.loc[:, int_df.columns[(int_df.min()>=0) &(int_df.max() >= 255) & (int_df.max()<65535)]] \\\n",
    "    .astype(np.uint16, copy=False)\n",
    "    int_df.info(verbose=False)\n",
    "    print(\"=============================================================\")\n",
    "\n",
    "    int_df[int_df.columns[(int_df.min()>=0) &(int_df.max() >= 65535) & (int_df.max()<4294967295)]] = \\\n",
    "    int_df.loc[:, int_df.columns[(int_df.min()>=0) &(int_df.max() >= 65535) & (int_df.max()<4294967295)]] \\\n",
    "    .astype(np.uint32, copy=False)\n",
    "    int_df.info(verbose=False)\n",
    "    print(\"=============================================================\")\n",
    "    print(\"Start working on floats...\")\n",
    "    float_df = float_df.astype(np.float32)\n",
    "    float_df.info(verbose=False)\n",
    "\n",
    "#     props[props.columns[(props.min()>=0) &(props.max() >= 4294967295)]] = \\\n",
    "#     props.loc[:, props.columns[(props.min()>=0) &(props.max() >= 4294967295)]] \\\n",
    "#     .astype(np.uint64, copy=False)\n",
    "#     props.info(verbose=False)\n",
    "#     print(\"=============================================================\")\n",
    "    print(\"Combining ints & floats...\")\n",
    "    props = pd.concat([int_df, float_df], axis = 'columns')\n",
    "    print(\"Complete\")\n",
    "    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n",
    "    mem_usg = props.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage is: \",mem_usg,\" MB\")\n",
    "    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n",
    "    \n",
    "    props.dtypes.to_csv('./final_df_dtypes.csv')\n",
    "    print(\"./final_df_dtypes.csv was exported\")\n",
    "    \n",
    "    return props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Do this only once for each dataset, then use the exported dtype series.\n",
    "# final_df = reduce_mem_usage3(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the dtypes Series to csv for future use\n",
    "final_df.dtypes.to_csv('../../../BDSE12-Group3/datasets/homecdt_ss_output/ss_fteng_fromBDSE12_03G_HomeCredit_V1_20200201b_dtypes_series.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_df['TARGET'].isna().sum(), \n",
    "      final_df['TARGET'].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "\n",
    "def kfold_lightgbm(df, num_folds = 5, stratified = True, debug= False):\n",
    "    # Divide in training/validation and test data\n",
    "    train_df = df[df['TARGET'].notnull()]\n",
    "    test_df = df[df['TARGET'].isnull()]\n",
    "    print(\"Starting LightGBM goss. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
    "    del df\n",
    "    gc.collect()\n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=924)\n",
    "    else:\n",
    "        folds = KFold(n_splits= num_folds, shuffle=True, random_state=924)\n",
    "    # Create arrays and dataframes to store results\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
    "        dtrain = lgb.Dataset(data=train_df[feats].iloc[train_idx], \n",
    "                             label=train_df['TARGET'].iloc[train_idx], \n",
    "                             free_raw_data=False, silent=True)\n",
    "        dvalid = lgb.Dataset(data=train_df[feats].iloc[valid_idx], \n",
    "                             label=train_df['TARGET'].iloc[valid_idx], \n",
    "                             free_raw_data=False, silent=True)\n",
    "\n",
    "        # LightGBM parameters found by Bayesian optimization\n",
    "        params = {\n",
    "            'objective': 'binary',\n",
    "            'boosting_type': 'goss',\n",
    "            'nthread': 4,\n",
    "            'learning_rate': 0.05,  # 02,\n",
    "            'num_leaves': 21, #20,33\n",
    "            'tree_learner': 'voting',\n",
    "            'colsample_bytree': 0.9497036,\n",
    "            'subsample': 0.8715623,\n",
    "            'subsample_freq': 0,\n",
    "            'max_depth': 7, #8,7\n",
    "            'reg_alpha': 0.041545473,\n",
    "            'reg_lambda': 0.0735294,\n",
    "            'min_split_gain': 0.0222415,\n",
    "            'min_data_in_leaf': 50, # ss add\n",
    "#             'min_child_weight': 1, # 60,39\n",
    "            'seed': 924,\n",
    "            'verbose': 2000,\n",
    "            'metric': 'auc',\n",
    "            'max_bin': 127,\n",
    "#             'histogram_pool_size': 20480\n",
    "#             'device' : 'gpu',\n",
    "#             'gpu_platform_id': 0,\n",
    "#             'gpu_device_id':0\n",
    "        }\n",
    "        \n",
    "        clf = lgb.train(\n",
    "            params=params,\n",
    "            train_set=dtrain,\n",
    "            num_boost_round=20000,\n",
    "            valid_sets=[dtrain, dvalid],\n",
    "            early_stopping_rounds=100,\n",
    "            verbose_eval=2000\n",
    "        )\n",
    "\n",
    "        oof_preds[valid_idx] = clf.predict(dvalid.data)\n",
    "        sub_preds += clf.predict(test_df[feats]) / folds.n_splits\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(dvalid.label, oof_preds[valid_idx])))\n",
    "        del clf, dtrain, dvalid\n",
    "        gc.collect()\n",
    "\n",
    "    print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))\n",
    "    # Write submission file and plot feature importance\n",
    "    if not debug:\n",
    "        sub_df = test_df[['SK_ID_CURR']].copy()\n",
    "        sub_df['TARGET'] = sub_preds\n",
    "        sub_df[['SK_ID_CURR', 'TARGET']].to_csv('../../../BDSE12-Group3/datasets/homecdt_ss_output/homecdt_submission_LGBM.csv', index= False)\n",
    "    display_importances(feature_importance_df)\n",
    "    return feature_importance_df\n",
    "\n",
    "# Display/plot feature importance\n",
    "def display_importances(feature_importance_df_):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout\n",
    "    plt.savefig('../../../BDSE12-Group3/datasets/homecdt_ss_output/lgbm_importances01.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## boosting_type：goss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time.time()\n",
    "kfold_lightgbm(final_df)\n",
    "print(\"Elapsed time={:5.2f} sec.\".format(time.time() - init_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## boosting_type：gbdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time.time()\n",
    "kfold_lightgbm(final_df, 10)\n",
    "print(\"Elapsed time={:5.2f} sec.\".format(time.time() - init_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## boosting_type：dart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time.time()\n",
    "kfold_lightgbm(final_df,10)\n",
    "print(\"Elapsed time={:5.2f} sec.\".format(time.time() - init_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## boosting_type：rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time.time()\n",
    "kfold_lightgbm(final_df,10)\n",
    "print(\"Elapsed time={:5.2f} sec.\".format(time.time() - init_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_xgb(df, num_folds, stratified = True, debug= False):\n",
    "    # Divide in training/validation and test data\n",
    "    train_df = df[df['TARGET'].notnull()]\n",
    "    test_df = df[df['TARGET'].isnull()]\n",
    "    print(\"Starting XGBoost. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
    "    del df\n",
    "    gc.collect()\n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=1054)\n",
    "    else:\n",
    "        folds = KFold(n_splits= num_folds, shuffle=True, random_state=1054)\n",
    "    # Create arrays and dataframes to store results\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
    "        #if n_fold == 0: # REmove for full K-fold run\n",
    "        cuda.select_device(0)\n",
    "        cuda.close()\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n",
    "\n",
    "        clf = XGBClassifier(learning_rate =0.01, \n",
    "                            n_estimators=5000, \n",
    "                            max_depth=4, \n",
    "                            min_child_weight=5,\n",
    "#                             tree_method='gpu_hist',\n",
    "                            subsample=0.8, \n",
    "                            colsample_bytree=0.8, \n",
    "                            objective= 'binary:logistic',\n",
    "                            nthread=4,\n",
    "                            scale_pos_weight=2.5,\n",
    "                            seed=28,\n",
    "                            reg_lambda = 1.2)\n",
    "        \n",
    "#         clf = pickle.load(open('test.pickle','rb'))\n",
    "        \n",
    "        cuda.select_device(0)\n",
    "        cuda.close()\n",
    "        \n",
    "        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "            eval_metric= 'auc', verbose= 1000, early_stopping_rounds= 200)\n",
    "        \n",
    "        cuda.select_device(0)\n",
    "        cuda.close()\n",
    "        \n",
    "        oof_preds[valid_idx] = clf.predict_proba(valid_x)[:, 1]\n",
    "        sub_preds += clf.predict_proba(test_df[feats])[:, 1] # / folds.n_splits # - Uncomment for K-fold \n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "        np.save(\"xgb_oof_preds_1\", oof_preds)\n",
    "        np.save(\"xgb_sub_preds_1\", sub_preds)\n",
    "        \n",
    "        cuda.select_device(0)\n",
    "        cuda.close()\n",
    "        \n",
    "    \n",
    "    clf = pickle.load(open('test.pickle','rb'))\n",
    "    # print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))\n",
    "    # Write submission file and plot feature importance\n",
    "    if not debug:\n",
    "        test_df['TARGET'] = sub_preds\n",
    "        test_df[['SK_ID_CURR', 'TARGET']].to_csv('submission_XGBoost_GPU.csv', index= False)\n",
    "    #display_importances(feature_importance_df)\n",
    "    #return feature_importance_df\n",
    "\n",
    "# Display/plot feature importance\n",
    "def display_importances(feature_importance_df_):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('XGBoost Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('xgb_importances02.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time.time()\n",
    "kfold_xgb(final_df, 5)\n",
    "print(\"Elapsed time={:5.2f} sec.\".format(time.time() - init_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
