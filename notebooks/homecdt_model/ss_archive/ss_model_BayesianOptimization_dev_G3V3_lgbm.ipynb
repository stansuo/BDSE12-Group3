{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge bayesian-optimization\n",
    "# conda install -c conda-forge xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from lightgbm import LGBMClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "# from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the saved dtypes Series\n",
    "final_df_dtypes = \\\n",
    "pd.read_csv('../../../BDSE12-Group3/datasets/homecdt_ss_output/ss_fteng_G3V2_ohe_recip_20200216a_dtypes_series.csv'\\\n",
    "            , header=None, index_col=0, squeeze=True)\n",
    "del final_df_dtypes.index.name\n",
    "final_df_dtypes = final_df_dtypes.to_dict()\n",
    "\n",
    "final_df = \\\n",
    "pd.read_csv('../../../BDSE12-Group3/datasets/homecdt_ss_output/ss_fteng_G3V2_ohe_recip_20200216a.csv'\\\n",
    "           , dtype= final_df_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in final_df.columns]\n",
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final_df\n",
    "del final_df\n",
    "gc.collect()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TARGET'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale_pos_weight \n",
    "282686 / 24825"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del bo\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_evaluate(**params):\n",
    "    warnings.simplefilter('ignore')\n",
    "    \n",
    "    params['num_leaves'] = int(params['num_leaves'])\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['min_child_samples'] = int(params['min_child_samples'])\n",
    "    params['max_bin'] = int(params['max_bin'])\n",
    "#     params['max_drop'] = int(params['max_drop'])\n",
    "    \n",
    "        \n",
    "    clf = LGBMClassifier(**params, \n",
    "#                          n_estimators = 2000,\n",
    "#                          nthread = 2, \n",
    "                         boosting_type='goss',\n",
    "#                          drop_seed = 924,\n",
    "                         objective='binary',\n",
    "                         scale_pos_weight = 11.387150050352467,\n",
    "                         random_state = 924,\n",
    "                         n_jobs = 2,\n",
    "                         silent = False,\n",
    "#                          importance_type (string, optional (default='split')) – \n",
    "#                          The type of feature importance to be filled into feature_importances_. \n",
    "#                          If ‘split’, result contains numbers of times the feature is used in a model. \n",
    "#                          If ‘gain’, result contains total gains of splits which use the feature.\n",
    "                        )\n",
    "\n",
    "    train_df = df[df['TARGET'].notnull()]\n",
    "    test_df = df[df['TARGET'].isnull()]\n",
    "\n",
    "    folds = StratifiedKFold(n_splits= 10, shuffle=True, random_state=1001)\n",
    "        \n",
    "    test_pred_proba = np.zeros(train_df.shape[0])\n",
    "    \n",
    "    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n",
    "\n",
    "        clf.fit(train_x, train_y, \n",
    "                eval_set = [(train_x, train_y), (valid_x, valid_y)], eval_metric = 'auc', \n",
    "                verbose = False, early_stopping_rounds = 200)\n",
    "\n",
    "        test_pred_proba[valid_idx] = clf.predict_proba(valid_x, num_iteration = clf.best_iteration_)[:, 1]\n",
    "        \n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    return roc_auc_score(train_df['TARGET'], test_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time.time()\n",
    "params = { \n",
    "          'num_leaves': (7, 161), \n",
    "          'max_depth': (7, 189),\n",
    "          'learning_rate': (.001, .1),\n",
    "#           'n_estimators':(50, 1000),\n",
    "#           'subsample_for_bin':(50000, 1000000),\n",
    "#           'top_rate':(0.0 ,1.0),\n",
    "          'min_split_gain': (.01, 1000),\n",
    "#           'drop_rate': (0.0, 1.0),\n",
    "#           'max_drop': (28, 343),\n",
    "#           'skip_drop': (0.0, 1.0),\n",
    "          'min_child_weight': (0.001, 1000),\n",
    "          'min_child_samples': (99, 9999),\n",
    "#         subsample (float, optional (default=1.)) – Subsample ratio of the training instance.\n",
    "#         subsample_freq (int, optional (default=0)) – Frequence of subsample, <=0 means no enable.\n",
    "#         colsample_bytree (float, optional (default=1.)) – Subsample ratio of columns when constructing each tree.\n",
    "          'reg_alpha': (.00, 10.0), \n",
    "          'reg_lambda': (.00, 10.0), \n",
    "          'max_bin': (63, 511)}\n",
    "bo = BayesianOptimization(lgbm_evaluate, params)\n",
    "bo.maximize(init_points = 7, n_iter = 7)\n",
    "print(\"Elapsed time={:5.2f} sec.\".format(time.time() - init_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list = bo.res\n",
    "len(params_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 輸出參數檔案\n",
    "import json\n",
    "with open('../../../BDSE12-Group3/datasets/homecdt_ss_output/params_list_BayesOpt_20200216a_G3V2_ohe_recip_lgbm.txt', 'w', encoding='utf-8') as fout:\n",
    "    for params in params_list:\n",
    "        json.dump(params, fout) \n",
    "        fout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_list[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need for looping\n",
    "# params_list = [bo.res[i] for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取參數檔案\n",
    "with open('../../../BDSE12-Group3/datasets/homecdt_ss_output/params_list_BayesOpt_20200210a.txt', 'r', encoding='utf-8') as f:\n",
    "    params_list_read = list(map(json.loads,f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list_read[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(params_list_read[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time.time()\n",
    "params = {'learning_rate': (.0, .1), \n",
    "          'num_leaves': (20, 100), \n",
    "          'subsample': (.0, 1.0), \n",
    "          'max_depth': (6, 9), \n",
    "          'reg_alpha': (.00, 1.0), \n",
    "          'reg_lambda': (.00, 1.0), \n",
    "          'min_split_gain': (.0, .1),\n",
    "          'min_child_weight': (20, 70)}\n",
    "bo = BayesianOptimization(lgbm_evaluate, params)\n",
    "bo.maximize(init_points = 5, n_iter = 10)\n",
    "print(\"Elapsed time={:5.2f} sec.\".format(time.time() - init_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_evaluate(**params):\n",
    "    warnings.simplefilter('ignore')\n",
    "    \n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "        \n",
    "    clf = XGBClassifier(**params, \n",
    "                        n_estimators = 2000, \n",
    "                        nthread = 5, \n",
    "                        objective= 'binary:logistic')\n",
    "\n",
    "    train_df = df[df['TARGET'].notnull()]\n",
    "    test_df = df[df['TARGET'].isnull()]\n",
    "\n",
    "    folds = StratifiedKFold(n_splits= 5, shuffle=True, random_state=1001)\n",
    "        \n",
    "    test_pred_proba = np.zeros(train_df.shape[0])\n",
    "    \n",
    "    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n",
    "\n",
    "        clf.fit(train_x, train_y, \n",
    "                eval_set = [(train_x, train_y), (valid_x, valid_y)], eval_metric = 'auc', \n",
    "                verbose = False, early_stopping_rounds = 100)\n",
    "\n",
    "        test_pred_proba[valid_idx] = clf.predict_proba(valid_x, num_iteration = clf.best_iteration_)[:, 1]\n",
    "        \n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    return roc_auc_score(train_df['TARGET'], test_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time.time()\n",
    "params = {'learning_rate': (.01, .03), \n",
    "          'subsample': (.0, 1.0), \n",
    "          'max_depth': (4, 9), \n",
    "          'reg_alpha': (.0, 1.0), \n",
    "          'reg_lambda': (.0, 1.0), \n",
    "          'scale_pos_weight': (.0, 5.0),\n",
    "          'colsample_bytree': (.0, 1.0)}\n",
    "bo = BayesianOptimization(xgb_evaluate, params)\n",
    "bo.maximize(init_points = 5, n_iter = 5)\n",
    "print(\"Elapsed time={:5.2f} sec.\".format(time.time() - init_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
