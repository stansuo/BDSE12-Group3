{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.core.frame import DataFrame\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = os.path.join(os.pardir, '..\\\\datasets\\\\homeCredit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\..\\datasets\\homeCredit\n"
     ]
    }
   ],
   "source": [
    "print(input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input files:\n",
      "['application_test.csv', 'application_train.csv', 'application_train_cleaning_v1.csv', 'BDSE12_03G_HomeCredit_V2.csv', 'bureau.csv', 'bureau_balance.csv', 'credit_card_balance.csv', 'final_test.csv', 'final_train.csv', 'final_train_test.csv', 'HomeCredit_columns_description.csv', 'installments_payments.csv', 'POS_CASH_balance.csv', 'previous_application.csv', 'rf_label.csv', 'rf_test.csv', 'rf_train.csv', 'sample_submission.csv']\n",
      "Loading data sets...\n",
      "Data loaded.\n",
      "Main application training data set shape = (307511, 122)\n",
      "Main application test data set shape = (48744, 121)\n",
      "Positive target proportion = 0.08\n"
     ]
    }
   ],
   "source": [
    "print('Input files:\\n{}'.format(os.listdir(input_dir)))\n",
    "print('Loading data sets...')\n",
    "\n",
    "sample_size = None\n",
    "app_train_df = pd.read_csv(os.path.join(input_dir, 'application_train.csv'), nrows=sample_size)\n",
    "app_test_df = pd.read_csv(os.path.join(input_dir, 'application_test.csv'), nrows=sample_size)\n",
    "bureau_df = pd.read_csv(os.path.join(input_dir, 'bureau.csv'), nrows=sample_size)\n",
    "bureau_balance_df = pd.read_csv(os.path.join(input_dir, 'bureau_balance.csv'), nrows=sample_size)\n",
    "credit_card_df = pd.read_csv(os.path.join(input_dir, 'credit_card_balance.csv'), nrows=sample_size)\n",
    "pos_cash_df = pd.read_csv(os.path.join(input_dir, 'POS_CASH_balance.csv'), nrows=sample_size)\n",
    "prev_app_df = pd.read_csv(os.path.join(input_dir, 'previous_application.csv'), nrows=sample_size)\n",
    "install_df = pd.read_csv(os.path.join(input_dir, 'installments_payments.csv'), nrows=sample_size)\n",
    "print('Data loaded.\\nMain application training data set shape = {}'.format(app_train_df.shape))\n",
    "print('Main application test data set shape = {}'.format(app_test_df.shape))\n",
    "print('Positive target proportion = {:.2f}'.format(app_train_df['TARGET'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>...</th>\n",
       "      <th>FLAG_DOCUMENT_18</th>\n",
       "      <th>FLAG_DOCUMENT_19</th>\n",
       "      <th>FLAG_DOCUMENT_20</th>\n",
       "      <th>FLAG_DOCUMENT_21</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_HOUR</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_DAY</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_WEEK</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>100002</td>\n",
       "      <td>1</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>100003</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>35698.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>100004</td>\n",
       "      <td>0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>100006</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>312682.5</td>\n",
       "      <td>29686.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>100007</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>21865.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  TARGET NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR  \\\n",
       "0      100002       1         Cash loans           M            N   \n",
       "1      100003       0         Cash loans           F            N   \n",
       "2      100004       0    Revolving loans           M            Y   \n",
       "3      100006       0         Cash loans           F            N   \n",
       "4      100007       0         Cash loans           M            N   \n",
       "\n",
       "  FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
       "0               Y             0          202500.0    406597.5      24700.5   \n",
       "1               N             0          270000.0   1293502.5      35698.5   \n",
       "2               Y             0           67500.0    135000.0       6750.0   \n",
       "3               Y             0          135000.0    312682.5      29686.5   \n",
       "4               Y             0          121500.0    513000.0      21865.5   \n",
       "\n",
       "   ...  FLAG_DOCUMENT_18 FLAG_DOCUMENT_19 FLAG_DOCUMENT_20 FLAG_DOCUMENT_21  \\\n",
       "0  ...                 0                0                0                0   \n",
       "1  ...                 0                0                0                0   \n",
       "2  ...                 0                0                0                0   \n",
       "3  ...                 0                0                0                0   \n",
       "4  ...                 0                0                0                0   \n",
       "\n",
       "  AMT_REQ_CREDIT_BUREAU_HOUR AMT_REQ_CREDIT_BUREAU_DAY  \\\n",
       "0                        0.0                       0.0   \n",
       "1                        0.0                       0.0   \n",
       "2                        0.0                       0.0   \n",
       "3                        NaN                       NaN   \n",
       "4                        0.0                       0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_WEEK  AMT_REQ_CREDIT_BUREAU_MON  \\\n",
       "0                         0.0                        0.0   \n",
       "1                         0.0                        0.0   \n",
       "2                         0.0                        0.0   \n",
       "3                         NaN                        NaN   \n",
       "4                         0.0                        0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_QRT  AMT_REQ_CREDIT_BUREAU_YEAR  \n",
       "0                        0.0                         1.0  \n",
       "1                        0.0                         0.0  \n",
       "2                        0.0                         0.0  \n",
       "3                        NaN                         NaN  \n",
       "4                        0.0                         0.0  \n",
       "\n",
       "[5 rows x 122 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(app_data, bureau_df, bureau_balance_df, credit_card_df,\n",
    "                        pos_cash_df, prev_app_df, install_df):\n",
    "    \"\"\" \n",
    "    Process the input dataframes into a single one containing all the features. Requires\n",
    "    a lot of aggregating of the supplementary datasets such that they have an entry per\n",
    "    customer.\n",
    "    \n",
    "    Also, add any new features created from the existing ones\n",
    "    \"\"\"\n",
    "    \n",
    "    # # Add new features\n",
    "    \n",
    "    # Amount loaned relative to salary\n",
    "    app_data['LOAN_INCOME_RATIO'] = app_data['AMT_CREDIT'] / app_data['AMT_INCOME_TOTAL']\n",
    "    app_data['ANNUITY_INCOME_RATIO'] = app_data['AMT_ANNUITY'] / app_data['AMT_INCOME_TOTAL']\n",
    "    app_data['ANNUITY LENGTH'] = app_data['AMT_CREDIT'] / app_data['AMT_ANNUITY']\n",
    "    \n",
    "    # # Aggregate and merge supplementary datasets\n",
    "    print('Combined train & test input shape before any merging  = {}'.format(app_data.shape))\n",
    "\n",
    "    # Previous applications\n",
    "    agg_funs = {'SK_ID_CURR': 'count', 'AMT_CREDIT': 'sum'}\n",
    "    prev_apps = prev_app_df.groupby('SK_ID_CURR').agg(agg_funs)\n",
    "    prev_apps.columns = ['PREV APP COUNT', 'TOTAL PREV LOAN AMT']\n",
    "    merged_df = app_data.merge(prev_apps, left_on='SK_ID_CURR', right_index=True, how='left')\n",
    "\n",
    "    # Average the rest of the previous app data\n",
    "    prev_apps_avg = prev_app_df.groupby('SK_ID_CURR').mean()\n",
    "    merged_df = merged_df.merge(prev_apps_avg, left_on='SK_ID_CURR', right_index=True,\n",
    "                                how='left', suffixes=['', '_PAVG'])\n",
    "    print('Shape after merging with previous apps num data = {}'.format(merged_df.shape))\n",
    "    \n",
    "    # Previous app categorical features\n",
    "    prev_app_df, cat_feats, _ = process_dataframe(prev_app_df)\n",
    "    prev_apps_cat_avg = prev_app_df[cat_feats + ['SK_ID_CURR']].groupby('SK_ID_CURR')\\\n",
    "                             .agg({k: lambda x: str(x.mode().iloc[0]) for k in cat_feats})\n",
    "    merged_df = merged_df.merge(prev_apps_cat_avg, left_on='SK_ID_CURR', right_index=True,\n",
    "                            how='left', suffixes=['', '_BAVG'])\n",
    "    print('Shape after merging with previous apps cat data = {}'.format(merged_df.shape))\n",
    "\n",
    "    # Credit card data - numerical features\n",
    "    wm = lambda x: np.average(x, weights=-1/credit_card_df.loc[x.index, 'MONTHS_BALANCE'])\n",
    "    credit_card_avgs = credit_card_df.groupby('SK_ID_CURR').agg(wm)   \n",
    "    merged_df = merged_df.merge(credit_card_avgs, left_on='SK_ID_CURR', right_index=True,\n",
    "                                how='left', suffixes=['', '_CCAVG'])\n",
    "    \n",
    "    # Credit card data - categorical features\n",
    "    most_recent_index = credit_card_df.groupby('SK_ID_CURR')['MONTHS_BALANCE'].idxmax()\n",
    "    cat_feats = credit_card_df.columns[credit_card_df.dtypes == 'object'].tolist()  + ['SK_ID_CURR']\n",
    "    merged_df = merged_df.merge(credit_card_df.loc[most_recent_index, cat_feats], left_on='SK_ID_CURR', right_on='SK_ID_CURR',\n",
    "                       how='left', suffixes=['', '_CCAVG'])\n",
    "    print('Shape after merging with credit card data = {}'.format(merged_df.shape))\n",
    "\n",
    "    # Credit bureau data - numerical features\n",
    "    credit_bureau_avgs = bureau_df.groupby('SK_ID_CURR').mean()\n",
    "    merged_df = merged_df.merge(credit_bureau_avgs, left_on='SK_ID_CURR', right_index=True,\n",
    "                                how='left', suffixes=['', '_BAVG'])\n",
    "    print('Shape after merging with credit bureau data = {}'.format(merged_df.shape))\n",
    "    \n",
    "    # Bureau balance data\n",
    "    most_recent_index = bureau_balance_df.groupby('SK_ID_BUREAU')['MONTHS_BALANCE'].idxmax()\n",
    "    bureau_balance_df = bureau_balance_df.loc[most_recent_index, :]\n",
    "    merged_df = merged_df.merge(bureau_balance_df, left_on='SK_ID_BUREAU', right_on='SK_ID_BUREAU',\n",
    "                            how='left', suffixes=['', '_B_B'])\n",
    "    print('Shape after merging with bureau balance data = {}'.format(merged_df.shape))\n",
    "\n",
    "    # Pos cash data - weight values by recency when averaging\n",
    "    wm = lambda x: np.average(x, weights=-1/pos_cash_df.loc[x.index, 'MONTHS_BALANCE'])\n",
    "    f = {'CNT_INSTALMENT': wm, 'CNT_INSTALMENT_FUTURE': wm, 'SK_DPD': wm, 'SK_DPD_DEF':wm}\n",
    "    cash_avg = pos_cash_df.groupby('SK_ID_CURR')['CNT_INSTALMENT','CNT_INSTALMENT_FUTURE',\n",
    "                                                 'SK_DPD', 'SK_DPD_DEF'].agg(f)\n",
    "    merged_df = merged_df.merge(cash_avg, left_on='SK_ID_CURR', right_index=True,\n",
    "                                how='left', suffixes=['', '_CAVG'])\n",
    "    \n",
    "    # Pos cash data data - categorical features\n",
    "    most_recent_index = pos_cash_df.groupby('SK_ID_CURR')['MONTHS_BALANCE'].idxmax()\n",
    "    cat_feats = pos_cash_df.columns[pos_cash_df.dtypes == 'object'].tolist()  + ['SK_ID_CURR']\n",
    "    merged_df = merged_df.merge(pos_cash_df.loc[most_recent_index, cat_feats], left_on='SK_ID_CURR', right_on='SK_ID_CURR',\n",
    "                       how='left', suffixes=['', '_CAVG'])\n",
    "    print('Shape after merging with pos cash data = {}'.format(merged_df.shape))\n",
    "\n",
    "    # Installments data\n",
    "    ins_avg = install_df.groupby('SK_ID_CURR').mean()\n",
    "    merged_df = merged_df.merge(ins_avg, left_on='SK_ID_CURR', right_index=True,\n",
    "                                how='left', suffixes=['', '_IAVG'])\n",
    "    print('Shape after merging with installments data = {}'.format(merged_df.shape))\n",
    "    \n",
    "    # Add more value counts\n",
    "    merged_df = merged_df.merge(pd.DataFrame(bureau_df['SK_ID_CURR'].value_counts()), left_on='SK_ID_CURR', \n",
    "                                right_index=True, how='left', suffixes=['', '_CNT_BUREAU'])\n",
    "    merged_df = merged_df.merge(pd.DataFrame(credit_card_df['SK_ID_CURR'].value_counts()), left_on='SK_ID_CURR', \n",
    "                                right_index=True, how='left', suffixes=['', '_CNT_CRED_CARD'])\n",
    "    merged_df = merged_df.merge(pd.DataFrame(pos_cash_df['SK_ID_CURR'].value_counts()), left_on='SK_ID_CURR', \n",
    "                                right_index=True, how='left', suffixes=['', '_CNT_POS_CASH'])\n",
    "    merged_df = merged_df.merge(pd.DataFrame(install_df['SK_ID_CURR'].value_counts()), left_on='SK_ID_CURR', \n",
    "                                right_index=True, how='left', suffixes=['', '_CNT_INSTALL'])\n",
    "    print('Shape after merging with counts data = {}'.format(merged_df.shape))\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(input_df, encoder_dict=None):\n",
    "    \"\"\" Process a dataframe into a form useable by LightGBM \"\"\"\n",
    "\n",
    "    # Label encode categoricals\n",
    "    categorical_feats = input_df.columns[input_df.dtypes == 'object']\n",
    "    categorical_feats = categorical_feats\n",
    "    encoder_dict = {}\n",
    "    for feat in categorical_feats:\n",
    "        encoder = LabelEncoder()\n",
    "        input_df[feat] = encoder.fit_transform(input_df[feat].fillna('NULL'))\n",
    "        encoder_dict[feat] = encoder\n",
    "\n",
    "    return input_df, categorical_feats.tolist(), encoder_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the datasets into a single one for training\n",
    "len_train = len(app_train_df)\n",
    "app_both = pd.concat([app_train_df, app_test_df])\n",
    "merged_df = feature_engineering(app_both, bureau_df, bureau_balance_df, credit_card_df,\n",
    "                                pos_cash_df, prev_app_df, install_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv('../../datasets/homeCredit/BDSE12_03G_HomeCredit_V2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.replace([np.inf, -np.inf], np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate metadata\n",
    "meta_cols = ['SK_ID_CURR']\n",
    "meta_df = merged_df[meta_cols]\n",
    "merged_df.drop(meta_cols, axis=1, inplace=True)\n",
    "\n",
    "# Process the data set.\n",
    "merged_df, categorical_feats, encoder_dict = process_dataframe(input_df=merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CODE_GENDER',\n",
       " 'FONDKAPREMONT_MODE',\n",
       " 'HOUSETYPE_MODE',\n",
       " 'NAME_CONTRACT_TYPE',\n",
       " 'NAME_EDUCATION_TYPE',\n",
       " 'NAME_FAMILY_STATUS',\n",
       " 'NAME_HOUSING_TYPE',\n",
       " 'NAME_INCOME_TYPE',\n",
       " 'NAME_TYPE_SUITE',\n",
       " 'OCCUPATION_TYPE',\n",
       " 'ORGANIZATION_TYPE',\n",
       " 'WALLSMATERIAL_MODE',\n",
       " 'WEEKDAY_APPR_PROCESS_START',\n",
       " 'HOUSE_CLUSTER_LABEL']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture other categorical features not as object data types:\n",
    "non_obj_categoricals = [\n",
    "    'FONDKAPREMONT_MODE',\n",
    "    'HOUR_APPR_PROCESS_START',\n",
    "    'HOUSETYPE_MODE',\n",
    "    'NAME_EDUCATION_TYPE',\n",
    "    'NAME_FAMILY_STATUS', \n",
    "    'NAME_HOUSING_TYPE',\n",
    "    'NAME_INCOME_TYPE', \n",
    "    'NAME_TYPE_SUITE', \n",
    "    'OCCUPATION_TYPE',\n",
    "    'ORGANIZATION_TYPE', \n",
    "    'WALLSMATERIAL_MODE',\n",
    "    'WEEKDAY_APPR_PROCESS_START', \n",
    "    'NAME_CONTRACT_TYPE_BAVG',\n",
    "    'WEEKDAY_APPR_PROCESS_START_BAVG',\n",
    "    'NAME_CASH_LOAN_PURPOSE', \n",
    "    'NAME_CONTRACT_STATUS', \n",
    "    'NAME_PAYMENT_TYPE',\n",
    "    'CODE_REJECT_REASON', \n",
    "    'NAME_TYPE_SUITE_BAVG', \n",
    "    'NAME_CLIENT_TYPE',\n",
    "    'NAME_GOODS_CATEGORY', \n",
    "    'NAME_PORTFOLIO', \n",
    "    'NAME_PRODUCT_TYPE',\n",
    "    'CHANNEL_TYPE', \n",
    "    'NAME_SELLER_INDUSTRY', \n",
    "    'NAME_YIELD_GROUP',\n",
    "    'PRODUCT_COMBINATION', \n",
    "    'NAME_CONTRACT_STATUS_CCAVG', \n",
    "    'STATUS',\n",
    "    'NAME_CONTRACT_STATUS_CAVG'\n",
    "]\n",
    "categorical_feats = categorical_feats + non_obj_categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract target before scaling\n",
    "labels = merged_df.pop('TARGET')\n",
    "labels = labels[:len_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape (one-hot)\n",
    "target = np.zeros([len(labels), len(np.unique(labels))])\n",
    "target[:, 0] = labels == 0\n",
    "target[:, 1] = labels == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns dropped for being over 80.0% null:\n",
      "RATE_INTEREST_PRIMARY_median\n",
      "RATE_INTEREST_PRIMARY_max\n",
      "RATE_INTEREST_PRIMARY_min\n",
      "RATE_INTEREST_PRIVILEGED_median\n",
      "RATE_INTEREST_PRIVILEGED_max\n",
      "RATE_INTEREST_PRIVILEGED_min\n",
      "RATE_INTEREST_PRIMARY_median_in2y\n",
      "RATE_INTEREST_PRIMARY_max_in2y\n",
      "RATE_INTEREST_PRIMARY_min_in2y\n",
      "RATE_INTEREST_PRIVILEGED_median_in2y\n",
      "RATE_INTEREST_PRIVILEGED_max_in2y\n",
      "RATE_INTEREST_PRIVILEGED_min_in2y\n",
      "DAYS_ENTRY_DIFF_MAX_max_RL\n",
      "DAYS_ENTRY_DIFF_MAX_median_RL\n",
      "DAYS_ENTRY_DIFF_MEAN_mean_RL\n",
      "AMT_PAYMENT_MEDIAN_median_RL\n",
      "AMT_PAYMENT_MEDIAN%_median_RL\n",
      "AMT_PAYMENT_MAX_median_RL\n",
      "AMT_PAYMENT_MAX_max_RL\n",
      "AMT_PAYMENT_MAX%_median_RL\n",
      "AMT_PAYMENT_MAX%_max_RL\n",
      "AMT_PAYMENT_MEDIAN/ANNUITY_median_RL\n",
      "AMT_PAYMENT_MAX/ANNUITY_median_RL\n",
      "AMT_PAYMENT_MAX/ANNUITY_max_RL\n",
      "AMT_PAYMENT_MEDIAN/CREDIT_median_RL\n",
      "AMT_PAYMENT_MAX/CREDIT_median_RL\n",
      "AMT_PAYMENT_MAX/CREDIT_max_RL\n",
      "PAYMENT_MAX/MEDIAN_max_RL\n",
      "PAYMENT_MAX/MEDIAN_min_RL\n",
      "PAYMENT_MAX/MEDIAN_median_RL\n",
      "DELAY%_mean_RL\n",
      "DELAY%_max_RL\n",
      "AMT_PAYMENT_ALL_mean_RL\n",
      "1_DELAY%_mean_RL\n",
      "1_DAYS_ENTRY_DIFF_MAX_mean_RL\n",
      "1_DAYS_ENTRY_DIFF_MAX_max_RL\n",
      "3_DELAY%_mean_RL\n",
      "3_DAYS_ENTRY_DIFF_MAX_mean_RL\n",
      "3_DAYS_ENTRY_DIFF_MAX_max_RL\n",
      "3_DAYS_ENTRY_DIFF_MEAN_mean_RL\n",
      "3_DAYS_ENTRY_DIFF_MEAN_max_RL\n",
      "6_DELAY%_mean_RL\n",
      "6_DAYS_ENTRY_DIFF_MAX_mean_RL\n",
      "6_DAYS_ENTRY_DIFF_MAX_max_RL\n",
      "6_DAYS_ENTRY_DIFF_MEAN_mean_RL\n",
      "6_DAYS_ENTRY_DIFF_MEAN_max_RL\n",
      "9_DELAY%_mean_RL\n",
      "9_DAYS_ENTRY_DIFF_MAX_mean_RL\n",
      "9_DAYS_ENTRY_DIFF_MAX_max_RL\n",
      "9_DAYS_ENTRY_DIFF_MEAN_mean_RL\n",
      "9_DAYS_ENTRY_DIFF_MEAN_max_RL\n",
      "-1_DELAY%_mean_RL\n",
      "-1_DAYS_ENTRY_DIFF_MAX_mean_RL\n",
      "-1_DAYS_ENTRY_DIFF_MAX_max_RL\n",
      "-3_DELAY%_mean_RL\n",
      "-3_DAYS_ENTRY_DIFF_MAX_mean_RL\n",
      "-3_DAYS_ENTRY_DIFF_MAX_max_RL\n",
      "-3_DAYS_ENTRY_DIFF_MEAN_mean_RL\n",
      "-3_DAYS_ENTRY_DIFF_MEAN_max_RL\n",
      "-6_DELAY%_mean_RL\n",
      "-6_DAYS_ENTRY_DIFF_MAX_mean_RL\n",
      "-6_DAYS_ENTRY_DIFF_MAX_max_RL\n",
      "-6_DAYS_ENTRY_DIFF_MEAN_mean_RL\n",
      "-6_DAYS_ENTRY_DIFF_MEAN_max_RL\n",
      "-9_DELAY%_mean_RL\n",
      "-9_DAYS_ENTRY_DIFF_MAX_mean_RL\n",
      "-9_DAYS_ENTRY_DIFF_MAX_max_RL\n",
      "-9_DAYS_ENTRY_DIFF_MEAN_mean_RL\n",
      "-9_DAYS_ENTRY_DIFF_MEAN_max_RL\n",
      "FINISHED%_RL\n",
      "CC_('AMT_DRAWINGS_ATM_MAX_CREDIT_LIMIT_MEDIAN _rate', 'max')_max\n",
      "CC_('AMT_DRAWINGS_OTHER_MAX_CREDIT_LIMIT_MEDIAN _rate', 'max')_max\n",
      "CC_('AMT_DRAWINGS_POS_MAX_CREDIT_LIMIT_MEDIAN _rate', 'max')_max\n",
      "CC_('AMT_DRAWINGS_CURRENT_MIN_rate_MAX', 'max')_max\n",
      "CC_('AMT_DRAWINGS_ATM_MIN_CREDIT_LIMIT_MEDIAN _rate', 'min')_min\n",
      "CC_('AMT_DRAWINGS_OTHER_MIN_CREDIT_LIMIT_MEDIAN _rate', 'min')_min\n",
      "CC_('AMT_DRAWINGS_POS_MIN_CREDIT_LIMIT_MEDIAN _rate', 'min')_min\n",
      "CC_('AMT_DRAWINGS_CURRENT_MIN', 'min')_min\n",
      "CC_('AMT_DRAWINGS_CURRENT_MIN_rate_MIN', 'min')_min\n",
      "CC_('AMT_PAYMENT_TOTAL_CURRENT_MIN_REGULARITY_mean_rate', 'median')_median\n",
      "CC_('AMT_PAYMENT_CURRENT_MIN_REGULARITY_mean_rate', 'median')_median\n",
      "CC_('AMT_DRAWINGS_CURRENT_CNT_CREDIT_LIMIT_rate', 'median')_median\n",
      "CC_('AMT_DRAWINGS_CURRENT_CNT_rate', 'median')_median\n",
      "AMT_REQ_CREDIT_BUREAU_HOUR/DAY\n",
      "AMT_REQ_CREDIT_BUREAU_HOUR/WEEK\n",
      "AMT_REQ_CREDIT_BUREAU_HOUR/MON\n",
      "AMT_REQ_CREDIT_BUREAU_HOUR/QRT\n",
      "AMT_REQ_CREDIT_BUREAU_DAY/WEEK\n",
      "AMT_REQ_CREDIT_BUREAU_DAY/MON\n",
      "AMT_REQ_CREDIT_BUREAU_DAY/QRT\n",
      "AMT_REQ_CREDIT_BUREAU_WEEK/MON\n"
     ]
    }
   ],
   "source": [
    "null_counts = merged_df.isnull().sum()\n",
    "null_counts = null_counts[null_counts > 0]\n",
    "null_ratios = null_counts / len(merged_df)\n",
    "\n",
    "# Drop columns over x% null\n",
    "null_thresh = .8\n",
    "null_cols = null_ratios[null_ratios > null_thresh].index\n",
    "merged_df.drop(null_cols, axis=1, inplace=True)\n",
    "print('Columns dropped for being over {}% null:'.format(100*null_thresh))\n",
    "for col in null_cols:\n",
    "    print(col)\n",
    "    if col in categorical_feats:\n",
    "        categorical_feats.pop(col)\n",
    "    \n",
    "# Fill the rest with the mean (TODO: do something better!)\n",
    "# merged_df.fillna(merged_df.median(), inplace=True)\n",
    "merged_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>column_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>CODE_GENDER</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>FONDKAPREMONT_MODE</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>HOUSETYPE_MODE</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>NAME_CONTRACT_TYPE</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>NAME_EDUCATION_TYPE</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               feature  column_index\n",
       "0          CODE_GENDER            19\n",
       "1   FONDKAPREMONT_MODE            74\n",
       "2       HOUSETYPE_MODE            76\n",
       "3   NAME_CONTRACT_TYPE            88\n",
       "4  NAME_EDUCATION_TYPE            89"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_feats_idx = np.array([merged_df.columns.get_loc(x) for x in categorical_feats])\n",
    "# int_feats_idx = [merged_df.columns.get_loc(x) for x in non_obj_categoricals]\n",
    "cat_feat_lookup = pd.DataFrame({'feature': categorical_feats, 'column_index': cat_feats_idx})\n",
    "cat_feat_lookup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>column_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Unnamed: 0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>AMT_ANNUITY</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>AMT_CREDIT</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>AMT_GOODS_PRICE</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>AMT_INCOME_TOTAL</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            feature  column_index\n",
       "0        Unnamed: 0             0\n",
       "1       AMT_ANNUITY             1\n",
       "2        AMT_CREDIT             2\n",
       "3   AMT_GOODS_PRICE             3\n",
       "4  AMT_INCOME_TOTAL             4"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_feats_idx = np.array(\n",
    "    [merged_df.columns.get_loc(x) \n",
    "     for x in merged_df.columns[~merged_df.columns.isin(categorical_feats)]]\n",
    ")\n",
    "cont_feat_lookup = pd.DataFrame(\n",
    "    {'feature': merged_df.columns[~merged_df.columns.isin(categorical_feats)], \n",
    "     'column_index': cont_feats_idx}\n",
    ")\n",
    "cont_feat_lookup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_DAY</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_HOUR</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_WEEK</th>\n",
       "      <th>...</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_QRT/YEAR</th>\n",
       "      <th>OUTSTANDING/CREDIT</th>\n",
       "      <th>OUTSTANDING/ANNUITY</th>\n",
       "      <th>PAYMENT_ALL_mean/CREDIT</th>\n",
       "      <th>ANNUITY_BU%</th>\n",
       "      <th>CREDIT%</th>\n",
       "      <th>ANNUITY/CREDIT%</th>\n",
       "      <th>ANNUITY_PREV%</th>\n",
       "      <th>CREDIT_PREV%</th>\n",
       "      <th>GOODS_PRICE_PREV%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>351000.0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.540155</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.903550</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35698.5</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>1129500.0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.417179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.613775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.316102</td>\n",
       "      <td>1.266184</td>\n",
       "      <td>1.27611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.157692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1071.428571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 705 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  AMT_ANNUITY  AMT_CREDIT  AMT_GOODS_PRICE  AMT_INCOME_TOTAL  \\\n",
       "0           0      24700.5    406597.5         351000.0          202500.0   \n",
       "1           1      35698.5   1293502.5        1129500.0          270000.0   \n",
       "2           2       6750.0    135000.0         135000.0           67500.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_DAY  AMT_REQ_CREDIT_BUREAU_HOUR  \\\n",
       "0                        0.0                         0.0   \n",
       "1                        0.0                         0.0   \n",
       "2                        0.0                         0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_MON  AMT_REQ_CREDIT_BUREAU_QRT  \\\n",
       "0                        0.0                        0.0   \n",
       "1                        0.0                        0.0   \n",
       "2                        0.0                        0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_WEEK  ...  AMT_REQ_CREDIT_BUREAU_QRT/YEAR  \\\n",
       "0                         0.0  ...                             0.0   \n",
       "1                         0.0  ...                             0.0   \n",
       "2                         0.0  ...                             0.0   \n",
       "\n",
       "   OUTSTANDING/CREDIT  OUTSTANDING/ANNUITY  PAYMENT_ALL_mean/CREDIT  \\\n",
       "0                 0.0                  0.0                 0.540155   \n",
       "1                 0.0                  0.0                 0.417179   \n",
       "2                 0.0                  0.0                 0.157692   \n",
       "\n",
       "   ANNUITY_BU%      CREDIT%  ANNUITY/CREDIT%  ANNUITY_PREV%  CREDIT_PREV%  \\\n",
       "0          inf     0.903550              inf            inf           inf   \n",
       "1          0.0     1.613775              0.0       0.316102      1.266184   \n",
       "2          0.0  1071.428571              0.0            inf           inf   \n",
       "\n",
       "   GOODS_PRICE_PREV%  \n",
       "0                inf  \n",
       "1            1.27611  \n",
       "2                inf  \n",
       "\n",
       "[3 rows x 705 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-69d9da5c9c0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmerged_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'merged_df' is not defined"
     ]
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-109-826f87e8cf05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfinal_col_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmerged_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmerged_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcont_feats_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcont_feats_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# scaler_2 = MinMaxScaler(feature_range=(0, 1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    551\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    637\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    661\u001b[0m         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,\n\u001b[0;32m    662\u001b[0m                         \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 663\u001b[1;33m                         force_all_finite='allow-nan')\n\u001b[0m\u001b[0;32m    664\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m         \u001b[1;31m# Even in the case of `with_mean=False`, we update the mean anyway\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[1;32m--> 542\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan)\u001b[0m\n\u001b[0;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[0;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infinity'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'NaN, infinity'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'object'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "final_col_names = merged_df.columns\n",
    "merged_df = merged_df.values\n",
    "merged_df[:, cont_feats_idx] = scaler.fit_transform(merged_df[:, cont_feats_idx])\n",
    "\n",
    "# scaler_2 = MinMaxScaler(feature_range=(0, 1))\n",
    "# merged_df[:, int_feats_idx] = scaler_2.fit_transform(merged_df[:, int_feats_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-9c7ebcbf6649>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Re-separate into labelled and unlabelled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpredict_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapp_train_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapp_test_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbureau_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbureau_balance_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcredit_card_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_cash_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev_app_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'merged_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Re-separate into labelled and unlabelled\n",
    "train_df = merged_df[:len_train]\n",
    "predict_df = merged_df[len_train:]\n",
    "del merged_df, app_train_df, app_test_df, bureau_df, bureau_balance_df, credit_card_df, pos_cash_df, prev_app_df\n",
    "gc.collect()\n",
    "\n",
    "# Create a validation set to check training performance\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_df, target, test_size=0.1, random_state=2, stratify=target[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of continous features:  164\n",
      "Number of categoricals pre-embedding:  65\n"
     ]
    }
   ],
   "source": [
    "# Fixed graph parameters\n",
    "# EMBEDDING_SIZE = 3  # Use cardinality / 2 instead\n",
    "N_HIDDEN_1 = 80\n",
    "N_HIDDEN_2 = 80\n",
    "N_HIDDEN_3 = 40\n",
    "n_cont_inputs = X_train[:, cont_feats_idx].shape[1]\n",
    "n_classes = 2\n",
    "\n",
    "# Learning parameters\n",
    "LEARNING_RATE = 0.01\n",
    "N_EPOCHS = 30\n",
    "N_ITERATIONS = 400\n",
    "BATCH_SIZE = 250\n",
    "\n",
    "print('Number of continous features: ', n_cont_inputs)\n",
    "print('Number of categoricals pre-embedding: ', X_train[:, cat_feats_idx].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_and_attach(X, X_cat, cardinality):  \n",
    "    embedding = tf.Variable(tf.random_uniform([cardinality, cardinality // 2], -1.0, 1.0))\n",
    "    embedded_x = tf.nn.embedding_lookup(embedding, X_cat)\n",
    "    return tf.concat([embedded_x, X], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Graph\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# Define placeholders for the categorical varaibles\n",
    "cat_placeholders, cat_cardinalities = [], []\n",
    "for idx in cat_feats_idx:\n",
    "    exec('X_cat_{} = tf.placeholder(tf.int32, shape=(None, ), name=\\'X_cat_{}\\')'.format(idx, idx))\n",
    "    exec('cat_placeholders.append(X_cat_{})'.format(idx))\n",
    "    cat_cardinalities.append(len(np.unique(np.concatenate([train_df[:, idx], \n",
    "                                                           predict_df[:, idx]], axis=0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Graph\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# Define placeholders for the categorical varaibles\n",
    "cat_placeholders, cat_cardinalities = [], []\n",
    "for idx in cat_feats_idx:\n",
    "    exec('X_cat_{} = tf.placeholder(tf.int32, shape=(None, ), name=\\'X_cat_{}\\')'.format(idx, idx))\n",
    "    exec('cat_placeholders.append(X_cat_{})'.format(idx))\n",
    "    cat_cardinalities.append(len(np.unique(np.concatenate([train_df[:, idx], \n",
    "                                                           predict_df[:, idx]], axis=0))))    \n",
    "# Other placeholders\n",
    "X_cont = tf.placeholder(tf.float32, shape=(None, n_cont_inputs), name='X_cont')\n",
    "y = tf.placeholder(tf.int32, shape=(None, n_classes), name='labels')\n",
    "train_mode = tf.placeholder(tf.bool)\n",
    "\n",
    "# Add embeddings to input\n",
    "X = tf.identity(X_cont)\n",
    "for feat, card in zip(cat_placeholders, cat_cardinalities):\n",
    "    X = embed_and_attach(X, feat, card)\n",
    "\n",
    "# Define the network layers. \n",
    "# Overfitting is a challenge so add L2 regularisation to weights in 1st layer & \n",
    "# a couple of dropout layers\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden_layer_1 = tf.layers.dense(inputs=X,\n",
    "                                     units=N_HIDDEN_1,\n",
    "                                     name='first_hidden_layer',\n",
    "                                     kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.3))\n",
    "    hidden_layer_1 = tf.layers.batch_normalization(hidden_layer_1, training=train_mode)\n",
    "    hidden_layer_1 = tf.nn.relu(hidden_layer_1)\n",
    "\n",
    "    drop_layer_1 = tf.layers.dropout(inputs=hidden_layer_1, \n",
    "                                     rate=0.4, \n",
    "                                     name='first_dropout_layer',\n",
    "                                     training=train_mode\n",
    "                                     \n",
    "    hidden_layer_2 = tf.layers.dense(inputs=drop_layer_1,\n",
    "                                     units=N_HIDDEN_2,\n",
    "                                     name='second_hidden_layer',\n",
    "                                     kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1))\n",
    "    hidden_layer_2 = tf.layers.batch_normalization(hidden_layer_2, training=train_mode)\n",
    "    hidden_layer_2 = tf.nn.relu(hidden_layer_2)\n",
    "                                     \n",
    "    drop_layer_2 = tf.layers.dropout(inputs=hidden_layer_2, \n",
    "                                     rate=0.2, \n",
    "                                     name='second_dropout_layer',\n",
    "                                     training=train_mode)\n",
    "\n",
    "    hidden_layer_3 = tf.layers.dense(inputs=drop_layer_2,\n",
    "                                     units=N_HIDDEN_3,\n",
    "                                     name='third_hidden_layer')\n",
    "    hidden_layer_3 = tf.layers.batch_normalization(hidden_layer_3, training=train_mode)\n",
    "    hidden_layer_3 = tf.nn.relu(hidden_layer_3)\n",
    "    \n",
    "    hidden_layer_4 = tf.layers.dense(inputs=hidden_layer_3,\n",
    "                                     units=N_HIDDEN_3,\n",
    "                                     name='fourth_hidden_layer')\n",
    "    hidden_layer_4 = tf.layers.batch_normalization(hidden_layer_4, training=train_mode)\n",
    "    hidden_layer_4 = tf.nn.relu(hidden_layer_4)\n",
    "\n",
    "    logits = tf.layers.dense(inputs=hidden_layer_4,\n",
    "                             units=n_classes,\n",
    "                             name='outputs')\n",
    "\n",
    "# Define the loss function for training as cross entropy\n",
    "with tf.name_scope('loss'):\n",
    "    xent = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xent, name='loss')\n",
    "\n",
    "# Define the optimiser\n",
    "with tf.name_scope('train'):\n",
    "    optimiser = tf.train.AdamOptimizer()  # AdagradOptimizer(learning_rate=LEARNING_RATE)\n",
    "    train_step = optimiser.minimize(loss)\n",
    "\n",
    "# Output the class probabilities to I can get the AUC\n",
    "with tf.name_scope('eval'):\n",
    "    predict = tf.argmax(logits, axis=1, name='class_predictions')\n",
    "    predict_proba = tf.nn.softmax(logits, name='probability_predictions')\n",
    "\n",
    "# Initialisation node and saver\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(cat_feats_idx, cat_placeholders, cont_feats_idx, batch_X, batch_y=None,\n",
    "                 training=False):\n",
    "    \"\"\" Return a feed dict for the graph including all the categorical features\n",
    "    to embed \"\"\"\n",
    "    \n",
    "    # Continuous X features and the labels if training run\n",
    "    feed_dict = {X_cont: batch_X[:, cont_feats_idx]}\n",
    "    if batch_y is not None:\n",
    "        feed_dict[y] = batch_y\n",
    "    \n",
    "    # Loop through the categorical features to provide values for the placeholders\n",
    "    for idx, tensor in zip(cat_feats_idx, cat_placeholders):\n",
    "        feed_dict[tensor] = batch_X[:, idx].reshape(-1, ).astype(int)\n",
    "        \n",
    "    # Training mode or not\n",
    "    feed_dict[train_mode] = training\n",
    "        \n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 400 iterations over 30 epochs with batchsize 250 ...\n",
      "Epoch = 0, Train AUC = 0.73619328, Valid AUC = 0.72566055\n",
      "Epoch = 1, Train AUC = 0.74818579, Valid AUC = 0.73445487\n",
      "Epoch = 2, Train AUC = 0.75400312, Valid AUC = 0.73864669\n",
      "Epoch = 3, Train AUC = 0.75752210, Valid AUC = 0.74348639\n",
      "Epoch = 4, Train AUC = 0.75969024, Valid AUC = 0.74502640\n",
      "Epoch = 5, Train AUC = 0.76406315, Valid AUC = 0.74647632\n",
      "Epoch = 6, Train AUC = 0.76602364, Valid AUC = 0.74748479\n",
      "Epoch = 7, Train AUC = 0.76749858, Valid AUC = 0.75023248\n",
      "Epoch = 8, Train AUC = 0.77163124, Valid AUC = 0.75298089\n",
      "Epoch = 9, Train AUC = 0.77244582, Valid AUC = 0.75138514\n",
      "Epoch = 10, Train AUC = 0.77326467, Valid AUC = 0.75082746\n",
      "Epoch = 11, Train AUC = 0.77685033, Valid AUC = 0.75335110\n",
      "Epoch = 12, Train AUC = 0.77669939, Valid AUC = 0.75318769\n",
      "Epoch = 13, Train AUC = 0.78044679, Valid AUC = 0.75490285\n",
      "Epoch = 14, Train AUC = 0.78119993, Valid AUC = 0.75608580\n",
      "Epoch = 15, Train AUC = 0.78323622, Valid AUC = 0.75524088\n",
      "Epoch = 16, Train AUC = 0.78591464, Valid AUC = 0.75696138\n",
      "Epoch = 17, Train AUC = 0.78685553, Valid AUC = 0.75615487\n",
      "Epoch = 18, Train AUC = 0.78746153, Valid AUC = 0.75732925\n",
      "Epoch = 19, Train AUC = 0.78928150, Valid AUC = 0.75736723\n",
      "Epoch = 20, Train AUC = 0.79040477, Valid AUC = 0.75926844\n",
      "Epoch = 21, Train AUC = 0.78857876, Valid AUC = 0.75576251\n",
      "Epoch = 22, Train AUC = 0.79389095, Valid AUC = 0.75703098\n",
      "Epoch = 23, Train AUC = 0.79359585, Valid AUC = 0.75852633\n",
      "Early stopping due to no improvement after 3 epochs.\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "train_auc, valid_auc = [], []\n",
    "n_rounds_not_improved = 0\n",
    "early_stopping_epochs = 3\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    init.run()\n",
    "\n",
    "    # Begin epoch loop\n",
    "    print('Training for {} iterations over {} epochs with batchsize {} ...'\n",
    "          .format(N_ITERATIONS, N_EPOCHS, BATCH_SIZE))\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        \n",
    "        # Iteration loop\n",
    "        for iteration in range(N_ITERATIONS):\n",
    "\n",
    "            # Get random selection of data for batch GD. Upsample positive classes to make it\n",
    "            # balanced in the training batch\n",
    "            pos_ratio = 0.5\n",
    "            pos_idx = np.random.choice(np.where(y_train[:, 1] == 1)[0], \n",
    "                                       size=int(np.round(BATCH_SIZE*pos_ratio)))\n",
    "            neg_idx = np.random.choice(np.where(y_train[:, 1] == 0)[0], \n",
    "                                       size=int(np.round(BATCH_SIZE*(1-pos_ratio))))\n",
    "            idx = np.concatenate([pos_idx, neg_idx])\n",
    "            \n",
    "            # Run training\n",
    "            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            sess.run([train_step, extra_update_ops], \n",
    "                     feed_dict=get_feed_dict(cat_feats_idx, cat_placeholders, cont_feats_idx, \n",
    "                                             X_train[idx, :], y_train[idx, :], 1))\n",
    "\n",
    "        # Check on the AUC\n",
    "        y_pred_train, y_prob_train = sess.run(\n",
    "            [predict, predict_proba], feed_dict=get_feed_dict(\n",
    "                cat_feats_idx, cat_placeholders, cont_feats_idx, X_train, y_train, False))\n",
    "        train_auc.append(roc_auc_score(y_train[:, 1], y_prob_train[:, 1]))\n",
    "        \n",
    "        y_pred_val, y_prob_val = sess.run(\n",
    "            [predict, predict_proba], feed_dict=get_feed_dict(\n",
    "                cat_feats_idx, cat_placeholders, cont_feats_idx, X_valid, y_valid, False))\n",
    "        valid_auc.append(roc_auc_score(y_valid[:, 1], y_prob_val[:, 1]))\n",
    "        \n",
    "        # Early stopping\n",
    "        if epoch > 1:\n",
    "            best_epoch_so_far = np.argmax(valid_auc[:-1])\n",
    "            if valid_auc[epoch] <= valid_auc[best_epoch_so_far]:\n",
    "                n_rounds_not_improved += 1\n",
    "            else:\n",
    "                n_rounds_not_improved = 0       \n",
    "            if n_rounds_not_improved > early_stopping_epochs:\n",
    "                print('Early stopping due to no improvement after {} epochs.'\n",
    "                      .format(early_stopping_epochs))\n",
    "                break\n",
    "        print('Epoch = {}, Train AUC = {:.8f}, Valid AUC = {:.8f}'\n",
    "              .format(epoch, train_auc[epoch], valid_auc[epoch]))\n",
    "\n",
    "    # Once trained, make predictions\n",
    "    print('Training complete.')\n",
    "    y_prob = sess.run(predict_proba, feed_dict=get_feed_dict(\n",
    "        cat_feats_idx, cat_placeholders, cont_feats_idx, predict_df, None, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.832731</td>\n",
       "      <td>0.167269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.228280</td>\n",
       "      <td>0.771720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.800431</td>\n",
       "      <td>0.199569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.871248</td>\n",
       "      <td>0.128752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.249577</td>\n",
       "      <td>0.750423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48739</td>\n",
       "      <td>0.646017</td>\n",
       "      <td>0.353983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48740</td>\n",
       "      <td>0.471324</td>\n",
       "      <td>0.528676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48741</td>\n",
       "      <td>0.813051</td>\n",
       "      <td>0.186949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48742</td>\n",
       "      <td>0.820381</td>\n",
       "      <td>0.179619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48743</td>\n",
       "      <td>0.204797</td>\n",
       "      <td>0.795202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48744 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1\n",
       "0      0.832731  0.167269\n",
       "1      0.228280  0.771720\n",
       "2      0.800431  0.199569\n",
       "3      0.871248  0.128752\n",
       "4      0.249577  0.750423\n",
       "...         ...       ...\n",
       "48739  0.646017  0.353983\n",
       "48740  0.471324  0.528676\n",
       "48741  0.813051  0.186949\n",
       "48742  0.820381  0.179619\n",
       "48743  0.204797  0.795202\n",
       "\n",
       "[48744 rows x 2 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame(y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
