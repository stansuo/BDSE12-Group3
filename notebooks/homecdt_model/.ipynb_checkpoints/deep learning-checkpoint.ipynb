{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.core.frame import DataFrame\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = os.path.join(os.pardir, '..\\\\datasets\\\\homeCredit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\..\\datasets\\homeCredit\n"
     ]
    }
   ],
   "source": [
    "print(input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input files:\n",
      "['application_test.csv', 'application_train.csv', 'application_train_cleaning_v1.csv', 'BDSE12_03G_HomeCredit_V2.csv', 'bureau.csv', 'bureau_balance.csv', 'credit_card_balance.csv', 'final_test.csv', 'final_train.csv', 'final_train_test.csv', 'HomeCredit_columns_description.csv', 'installments_payments.csv', 'POS_CASH_balance.csv', 'previous_application.csv', 'rf_label.csv', 'rf_test.csv', 'rf_train.csv', 'sample_submission.csv']\n",
      "Loading data sets...\n",
      "Data loaded.\n",
      "Main application training data set shape = (307511, 122)\n",
      "Main application test data set shape = (48744, 121)\n",
      "Positive target proportion = 0.08\n"
     ]
    }
   ],
   "source": [
    "print('Input files:\\n{}'.format(os.listdir(input_dir)))\n",
    "print('Loading data sets...')\n",
    "\n",
    "sample_size = None\n",
    "app_train_df = pd.read_csv(os.path.join(input_dir, 'application_train.csv'), nrows=sample_size)\n",
    "app_test_df = pd.read_csv(os.path.join(input_dir, 'application_test.csv'), nrows=sample_size)\n",
    "bureau_df = pd.read_csv(os.path.join(input_dir, 'bureau.csv'), nrows=sample_size)\n",
    "bureau_balance_df = pd.read_csv(os.path.join(input_dir, 'bureau_balance.csv'), nrows=sample_size)\n",
    "credit_card_df = pd.read_csv(os.path.join(input_dir, 'credit_card_balance.csv'), nrows=sample_size)\n",
    "pos_cash_df = pd.read_csv(os.path.join(input_dir, 'POS_CASH_balance.csv'), nrows=sample_size)\n",
    "prev_app_df = pd.read_csv(os.path.join(input_dir, 'previous_application.csv'), nrows=sample_size)\n",
    "install_df = pd.read_csv(os.path.join(input_dir, 'installments_payments.csv'), nrows=sample_size)\n",
    "print('Data loaded.\\nMain application training data set shape = {}'.format(app_train_df.shape))\n",
    "print('Main application test data set shape = {}'.format(app_test_df.shape))\n",
    "print('Positive target proportion = {:.2f}'.format(app_train_df['TARGET'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>...</th>\n",
       "      <th>FLAG_DOCUMENT_18</th>\n",
       "      <th>FLAG_DOCUMENT_19</th>\n",
       "      <th>FLAG_DOCUMENT_20</th>\n",
       "      <th>FLAG_DOCUMENT_21</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_HOUR</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_DAY</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_WEEK</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>100002</td>\n",
       "      <td>1</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>100003</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>35698.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>100004</td>\n",
       "      <td>0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>100006</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>312682.5</td>\n",
       "      <td>29686.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>100007</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>21865.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  TARGET NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR  \\\n",
       "0      100002       1         Cash loans           M            N   \n",
       "1      100003       0         Cash loans           F            N   \n",
       "2      100004       0    Revolving loans           M            Y   \n",
       "3      100006       0         Cash loans           F            N   \n",
       "4      100007       0         Cash loans           M            N   \n",
       "\n",
       "  FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
       "0               Y             0          202500.0    406597.5      24700.5   \n",
       "1               N             0          270000.0   1293502.5      35698.5   \n",
       "2               Y             0           67500.0    135000.0       6750.0   \n",
       "3               Y             0          135000.0    312682.5      29686.5   \n",
       "4               Y             0          121500.0    513000.0      21865.5   \n",
       "\n",
       "   ...  FLAG_DOCUMENT_18 FLAG_DOCUMENT_19 FLAG_DOCUMENT_20 FLAG_DOCUMENT_21  \\\n",
       "0  ...                 0                0                0                0   \n",
       "1  ...                 0                0                0                0   \n",
       "2  ...                 0                0                0                0   \n",
       "3  ...                 0                0                0                0   \n",
       "4  ...                 0                0                0                0   \n",
       "\n",
       "  AMT_REQ_CREDIT_BUREAU_HOUR AMT_REQ_CREDIT_BUREAU_DAY  \\\n",
       "0                        0.0                       0.0   \n",
       "1                        0.0                       0.0   \n",
       "2                        0.0                       0.0   \n",
       "3                        NaN                       NaN   \n",
       "4                        0.0                       0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_WEEK  AMT_REQ_CREDIT_BUREAU_MON  \\\n",
       "0                         0.0                        0.0   \n",
       "1                         0.0                        0.0   \n",
       "2                         0.0                        0.0   \n",
       "3                         NaN                        NaN   \n",
       "4                         0.0                        0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_QRT  AMT_REQ_CREDIT_BUREAU_YEAR  \n",
       "0                        0.0                         1.0  \n",
       "1                        0.0                         0.0  \n",
       "2                        0.0                         0.0  \n",
       "3                        NaN                         NaN  \n",
       "4                        0.0                         0.0  \n",
       "\n",
       "[5 rows x 122 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(app_data, bureau_df, bureau_balance_df, credit_card_df,\n",
    "                        pos_cash_df, prev_app_df, install_df):\n",
    "    \"\"\" \n",
    "    Process the input dataframes into a single one containing all the features. Requires\n",
    "    a lot of aggregating of the supplementary datasets such that they have an entry per\n",
    "    customer.\n",
    "    \n",
    "    Also, add any new features created from the existing ones\n",
    "    \"\"\"\n",
    "    \n",
    "    # # Add new features\n",
    "    \n",
    "    # Amount loaned relative to salary\n",
    "    app_data['LOAN_INCOME_RATIO'] = app_data['AMT_CREDIT'] / app_data['AMT_INCOME_TOTAL']\n",
    "    app_data['ANNUITY_INCOME_RATIO'] = app_data['AMT_ANNUITY'] / app_data['AMT_INCOME_TOTAL']\n",
    "    app_data['ANNUITY LENGTH'] = app_data['AMT_CREDIT'] / app_data['AMT_ANNUITY']\n",
    "    \n",
    "    # # Aggregate and merge supplementary datasets\n",
    "    print('Combined train & test input shape before any merging  = {}'.format(app_data.shape))\n",
    "\n",
    "    # Previous applications\n",
    "    agg_funs = {'SK_ID_CURR': 'count', 'AMT_CREDIT': 'sum'}\n",
    "    prev_apps = prev_app_df.groupby('SK_ID_CURR').agg(agg_funs)\n",
    "    prev_apps.columns = ['PREV APP COUNT', 'TOTAL PREV LOAN AMT']\n",
    "    merged_df = app_data.merge(prev_apps, left_on='SK_ID_CURR', right_index=True, how='left')\n",
    "\n",
    "    # Average the rest of the previous app data\n",
    "    prev_apps_avg = prev_app_df.groupby('SK_ID_CURR').mean()\n",
    "    merged_df = merged_df.merge(prev_apps_avg, left_on='SK_ID_CURR', right_index=True,\n",
    "                                how='left', suffixes=['', '_PAVG'])\n",
    "    print('Shape after merging with previous apps num data = {}'.format(merged_df.shape))\n",
    "    \n",
    "    # Previous app categorical features\n",
    "    prev_app_df, cat_feats, _ = process_dataframe(prev_app_df)\n",
    "    prev_apps_cat_avg = prev_app_df[cat_feats + ['SK_ID_CURR']].groupby('SK_ID_CURR')\\\n",
    "                             .agg({k: lambda x: str(x.mode().iloc[0]) for k in cat_feats})\n",
    "    merged_df = merged_df.merge(prev_apps_cat_avg, left_on='SK_ID_CURR', right_index=True,\n",
    "                            how='left', suffixes=['', '_BAVG'])\n",
    "    print('Shape after merging with previous apps cat data = {}'.format(merged_df.shape))\n",
    "\n",
    "    # Credit card data - numerical features\n",
    "    wm = lambda x: np.average(x, weights=-1/credit_card_df.loc[x.index, 'MONTHS_BALANCE'])\n",
    "    credit_card_avgs = credit_card_df.groupby('SK_ID_CURR').agg(wm)   \n",
    "    merged_df = merged_df.merge(credit_card_avgs, left_on='SK_ID_CURR', right_index=True,\n",
    "                                how='left', suffixes=['', '_CCAVG'])\n",
    "    \n",
    "    # Credit card data - categorical features\n",
    "    most_recent_index = credit_card_df.groupby('SK_ID_CURR')['MONTHS_BALANCE'].idxmax()\n",
    "    cat_feats = credit_card_df.columns[credit_card_df.dtypes == 'object'].tolist()  + ['SK_ID_CURR']\n",
    "    merged_df = merged_df.merge(credit_card_df.loc[most_recent_index, cat_feats], left_on='SK_ID_CURR', right_on='SK_ID_CURR',\n",
    "                       how='left', suffixes=['', '_CCAVG'])\n",
    "    print('Shape after merging with credit card data = {}'.format(merged_df.shape))\n",
    "\n",
    "    # Credit bureau data - numerical features\n",
    "    credit_bureau_avgs = bureau_df.groupby('SK_ID_CURR').mean()\n",
    "    merged_df = merged_df.merge(credit_bureau_avgs, left_on='SK_ID_CURR', right_index=True,\n",
    "                                how='left', suffixes=['', '_BAVG'])\n",
    "    print('Shape after merging with credit bureau data = {}'.format(merged_df.shape))\n",
    "    \n",
    "    # Bureau balance data\n",
    "    most_recent_index = bureau_balance_df.groupby('SK_ID_BUREAU')['MONTHS_BALANCE'].idxmax()\n",
    "    bureau_balance_df = bureau_balance_df.loc[most_recent_index, :]\n",
    "    merged_df = merged_df.merge(bureau_balance_df, left_on='SK_ID_BUREAU', right_on='SK_ID_BUREAU',\n",
    "                            how='left', suffixes=['', '_B_B'])\n",
    "    print('Shape after merging with bureau balance data = {}'.format(merged_df.shape))\n",
    "\n",
    "    # Pos cash data - weight values by recency when averaging\n",
    "    wm = lambda x: np.average(x, weights=-1/pos_cash_df.loc[x.index, 'MONTHS_BALANCE'])\n",
    "    f = {'CNT_INSTALMENT': wm, 'CNT_INSTALMENT_FUTURE': wm, 'SK_DPD': wm, 'SK_DPD_DEF':wm}\n",
    "    cash_avg = pos_cash_df.groupby('SK_ID_CURR')['CNT_INSTALMENT','CNT_INSTALMENT_FUTURE',\n",
    "                                                 'SK_DPD', 'SK_DPD_DEF'].agg(f)\n",
    "    merged_df = merged_df.merge(cash_avg, left_on='SK_ID_CURR', right_index=True,\n",
    "                                how='left', suffixes=['', '_CAVG'])\n",
    "    \n",
    "    # Pos cash data data - categorical features\n",
    "    most_recent_index = pos_cash_df.groupby('SK_ID_CURR')['MONTHS_BALANCE'].idxmax()\n",
    "    cat_feats = pos_cash_df.columns[pos_cash_df.dtypes == 'object'].tolist()  + ['SK_ID_CURR']\n",
    "    merged_df = merged_df.merge(pos_cash_df.loc[most_recent_index, cat_feats], left_on='SK_ID_CURR', right_on='SK_ID_CURR',\n",
    "                       how='left', suffixes=['', '_CAVG'])\n",
    "    print('Shape after merging with pos cash data = {}'.format(merged_df.shape))\n",
    "\n",
    "    # Installments data\n",
    "    ins_avg = install_df.groupby('SK_ID_CURR').mean()\n",
    "    merged_df = merged_df.merge(ins_avg, left_on='SK_ID_CURR', right_index=True,\n",
    "                                how='left', suffixes=['', '_IAVG'])\n",
    "    print('Shape after merging with installments data = {}'.format(merged_df.shape))\n",
    "    \n",
    "    # Add more value counts\n",
    "    merged_df = merged_df.merge(pd.DataFrame(bureau_df['SK_ID_CURR'].value_counts()), left_on='SK_ID_CURR', \n",
    "                                right_index=True, how='left', suffixes=['', '_CNT_BUREAU'])\n",
    "    merged_df = merged_df.merge(pd.DataFrame(credit_card_df['SK_ID_CURR'].value_counts()), left_on='SK_ID_CURR', \n",
    "                                right_index=True, how='left', suffixes=['', '_CNT_CRED_CARD'])\n",
    "    merged_df = merged_df.merge(pd.DataFrame(pos_cash_df['SK_ID_CURR'].value_counts()), left_on='SK_ID_CURR', \n",
    "                                right_index=True, how='left', suffixes=['', '_CNT_POS_CASH'])\n",
    "    merged_df = merged_df.merge(pd.DataFrame(install_df['SK_ID_CURR'].value_counts()), left_on='SK_ID_CURR', \n",
    "                                right_index=True, how='left', suffixes=['', '_CNT_INSTALL'])\n",
    "    print('Shape after merging with counts data = {}'.format(merged_df.shape))\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(input_df, encoder_dict=None):\n",
    "    \"\"\" Process a dataframe into a form useable by LightGBM \"\"\"\n",
    "\n",
    "    # Label encode categoricals\n",
    "    categorical_feats = input_df.columns[input_df.dtypes == 'object']\n",
    "    categorical_feats = categorical_feats\n",
    "    encoder_dict = {}\n",
    "    for feat in categorical_feats:\n",
    "        encoder = LabelEncoder()\n",
    "        input_df[feat] = encoder.fit_transform(input_df[feat].fillna('NULL'))\n",
    "        encoder_dict[feat] = encoder\n",
    "\n",
    "    return input_df, categorical_feats.tolist(), encoder_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined train & test input shape before any merging  = (356255, 125)\n",
      "Shape after merging with previous apps num data = (356255, 147)\n",
      "Shape after merging with previous apps cat data = (356255, 163)\n",
      "Shape after merging with credit card data = (356255, 185)\n",
      "Shape after merging with credit bureau data = (356255, 198)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:1089: UserWarning: You are merging on int and float columns where the float values are not equal to their int representation\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after merging with bureau balance data = (356255, 200)\n",
      "Shape after merging with pos cash data = (356255, 205)\n",
      "Shape after merging with installments data = (356255, 212)\n",
      "Shape after merging with counts data = (356255, 216)\n"
     ]
    }
   ],
   "source": [
    "# Merge the datasets into a single one for training\n",
    "len_train = len(app_train_df)\n",
    "app_both = pd.concat([app_train_df, app_test_df])\n",
    "merged_df = feature_engineering(app_both, bureau_df, bureau_balance_df, credit_card_df,\n",
    "                                pos_cash_df, prev_app_df, install_df)\n",
    "\n",
    "# Separate metadata\n",
    "meta_cols = ['SK_ID_CURR', 'SK_ID_BUREAU', 'SK_ID_PREV']\n",
    "meta_df = merged_df[meta_cols]\n",
    "merged_df.drop(meta_cols, axis=1, inplace=True)\n",
    "\n",
    "# Process the data set.\n",
    "merged_df, categorical_feats, encoder_dict = process_dataframe(input_df=merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture other categorical features not as object data types:\n",
    "non_obj_categoricals = [\n",
    "    'FONDKAPREMONT_MODE',\n",
    "    'HOUR_APPR_PROCESS_START',\n",
    "    'HOUSETYPE_MODE',\n",
    "    'NAME_EDUCATION_TYPE',\n",
    "    'NAME_FAMILY_STATUS', \n",
    "    'NAME_HOUSING_TYPE',\n",
    "    'NAME_INCOME_TYPE', \n",
    "    'NAME_TYPE_SUITE', \n",
    "    'OCCUPATION_TYPE',\n",
    "    'ORGANIZATION_TYPE', \n",
    "    'WALLSMATERIAL_MODE',\n",
    "    'WEEKDAY_APPR_PROCESS_START', \n",
    "    'NAME_CONTRACT_TYPE_BAVG',\n",
    "    'WEEKDAY_APPR_PROCESS_START_BAVG',\n",
    "    'NAME_CASH_LOAN_PURPOSE', \n",
    "    'NAME_CONTRACT_STATUS', \n",
    "    'NAME_PAYMENT_TYPE',\n",
    "    'CODE_REJECT_REASON', \n",
    "    'NAME_TYPE_SUITE_BAVG', \n",
    "    'NAME_CLIENT_TYPE',\n",
    "    'NAME_GOODS_CATEGORY', \n",
    "    'NAME_PORTFOLIO', \n",
    "    'NAME_PRODUCT_TYPE',\n",
    "    'CHANNEL_TYPE', \n",
    "    'NAME_SELLER_INDUSTRY', \n",
    "    'NAME_YIELD_GROUP',\n",
    "    'PRODUCT_COMBINATION', \n",
    "    'NAME_CONTRACT_STATUS_CCAVG', \n",
    "    'STATUS',\n",
    "    'NAME_CONTRACT_STATUS_CAVG'\n",
    "]\n",
    "categorical_feats = categorical_feats + non_obj_categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract target before scaling\n",
    "labels = merged_df.pop('TARGET')\n",
    "labels = labels[:len_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape (one-hot)\n",
    "target = np.zeros([len(labels), len(np.unique(labels))])\n",
    "target[:, 0] = labels == 0\n",
    "target[:, 1] = labels == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-ffdb6bf7e17b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnull_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnull_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnull_counts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnull_counts\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnull_ratios\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnull_counts\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Drop columns over x% null\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'merged_df' is not defined"
     ]
    }
   ],
   "source": [
    "null_counts = merged_df.isnull().sum()\n",
    "null_counts = null_counts[null_counts > 0]\n",
    "null_ratios = null_counts / len(merged_df)\n",
    "\n",
    "# Drop columns over x% null\n",
    "null_thresh = .8\n",
    "null_cols = null_ratios[null_ratios > null_thresh].index\n",
    "merged_df.drop(null_cols, axis=1, inplace=True)\n",
    "print('Columns dropped for being over {}% null:'.format(100*null_thresh))\n",
    "for col in null_cols:\n",
    "    print(col)\n",
    "    if col in categorical_feats:\n",
    "        categorical_feats.pop(col)\n",
    "    \n",
    "# Fill the rest with the mean (TODO: do something better!)\n",
    "# merged_df.fillna(merged_df.median(), inplace=True)\n",
    "merged_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-19856f04dc52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcat_feats_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmerged_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcategorical_feats\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mint_feats_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmerged_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnon_obj_categoricals\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcat_feat_lookup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'feature'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcategorical_feats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'column_index'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcat_feats_idx\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcat_feat_lookup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-19856f04dc52>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcat_feats_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmerged_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcategorical_feats\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mint_feats_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmerged_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnon_obj_categoricals\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcat_feat_lookup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'feature'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcategorical_feats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'column_index'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcat_feats_idx\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcat_feat_lookup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'merged_df' is not defined"
     ]
    }
   ],
   "source": [
    "cat_feats_idx = np.array([merged_df.columns.get_loc(x) for x in categorical_feats])\n",
    "int_feats_idx = [merged_df.columns.get_loc(x) for x in non_obj_categoricals]\n",
    "cat_feat_lookup = pd.DataFrame({'feature': categorical_feats, 'column_index': cat_feats_idx})\n",
    "cat_feat_lookup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-d31e593d8ac8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m cont_feats_idx = np.array(\n\u001b[0;32m      2\u001b[0m     [merged_df.columns.get_loc(x) \n\u001b[1;32m----> 3\u001b[1;33m      for x in merged_df.columns[~merged_df.columns.isin(categorical_feats)]]\n\u001b[0m\u001b[0;32m      4\u001b[0m )\n\u001b[0;32m      5\u001b[0m cont_feat_lookup = pd.DataFrame(\n",
      "\u001b[1;31mNameError\u001b[0m: name 'merged_df' is not defined"
     ]
    }
   ],
   "source": [
    "cont_feats_idx = np.array(\n",
    "    [merged_df.columns.get_loc(x) \n",
    "     for x in merged_df.columns[~merged_df.columns.isin(categorical_feats)]]\n",
    ")\n",
    "cont_feat_lookup = pd.DataFrame(\n",
    "    {'feature': merged_df.columns[~merged_df.columns.isin(categorical_feats)], \n",
    "     'column_index': cont_feats_idx}\n",
    ")\n",
    "cont_feat_lookup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-fe2bcc9db65a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmerged_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'merged_df' is not defined"
     ]
    }
   ],
   "source": [
    "merged_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-69d9da5c9c0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmerged_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'merged_df' is not defined"
     ]
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-597bb6b8c698>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfinal_col_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmerged_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcont_feats_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcont_feats_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'merged_df' is not defined"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "final_col_names = merged_df.columns\n",
    "merged_df = merged_df.values\n",
    "merged_df[:, cont_feats_idx] = scaler.fit_transform(merged_df[:, cont_feats_idx])\n",
    "\n",
    "scaler_2 = MinMaxScaler(feature_range=(0, 1))\n",
    "merged_df[:, int_feats_idx] = scaler_2.fit_transform(merged_df[:, int_feats_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-9c7ebcbf6649>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Re-separate into labelled and unlabelled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpredict_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapp_train_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapp_test_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbureau_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbureau_balance_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcredit_card_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_cash_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev_app_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'merged_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Re-separate into labelled and unlabelled\n",
    "train_df = merged_df[:len_train]\n",
    "predict_df = merged_df[len_train:]\n",
    "del merged_df, app_train_df, app_test_df, bureau_df, bureau_balance_df, credit_card_df, pos_cash_df, prev_app_df\n",
    "gc.collect()\n",
    "\n",
    "# Create a validation set to check training performance\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_df, target, test_size=0.1, random_state=2, stratify=target[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of continous features:  164\n",
      "Number of categoricals pre-embedding:  65\n"
     ]
    }
   ],
   "source": [
    "# Fixed graph parameters\n",
    "# EMBEDDING_SIZE = 3  # Use cardinality / 2 instead\n",
    "N_HIDDEN_1 = 80\n",
    "N_HIDDEN_2 = 80\n",
    "N_HIDDEN_3 = 40\n",
    "n_cont_inputs = X_train[:, cont_feats_idx].shape[1]\n",
    "n_classes = 2\n",
    "\n",
    "# Learning parameters\n",
    "LEARNING_RATE = 0.01\n",
    "N_EPOCHS = 30\n",
    "N_ITERATIONS = 400\n",
    "BATCH_SIZE = 250\n",
    "\n",
    "print('Number of continous features: ', n_cont_inputs)\n",
    "print('Number of categoricals pre-embedding: ', X_train[:, cat_feats_idx].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_and_attach(X, X_cat, cardinality):  \n",
    "    embedding = tf.Variable(tf.random_uniform([cardinality, cardinality // 2], -1.0, 1.0))\n",
    "    embedded_x = tf.nn.embedding_lookup(embedding, X_cat)\n",
    "    return tf.concat([embedded_x, X], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Graph\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# Define placeholders for the categorical varaibles\n",
    "cat_placeholders, cat_cardinalities = [], []\n",
    "for idx in cat_feats_idx:\n",
    "    exec('X_cat_{} = tf.placeholder(tf.int32, shape=(None, ), name=\\'X_cat_{}\\')'.format(idx, idx))\n",
    "    exec('cat_placeholders.append(X_cat_{})'.format(idx))\n",
    "    cat_cardinalities.append(len(np.unique(np.concatenate([train_df[:, idx], \n",
    "                                                           predict_df[:, idx]], axis=0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Graph\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# Define placeholders for the categorical varaibles\n",
    "cat_placeholders, cat_cardinalities = [], []\n",
    "for idx in cat_feats_idx:\n",
    "    exec('X_cat_{} = tf.placeholder(tf.int32, shape=(None, ), name=\\'X_cat_{}\\')'.format(idx, idx))\n",
    "    exec('cat_placeholders.append(X_cat_{})'.format(idx))\n",
    "    cat_cardinalities.append(len(np.unique(np.concatenate([train_df[:, idx], \n",
    "                                                           predict_df[:, idx]], axis=0))))    \n",
    "# Other placeholders\n",
    "X_cont = tf.placeholder(tf.float32, shape=(None, n_cont_inputs), name='X_cont')\n",
    "y = tf.placeholder(tf.int32, shape=(None, n_classes), name='labels')\n",
    "train_mode = tf.placeholder(tf.bool)\n",
    "\n",
    "# Add embeddings to input\n",
    "X = tf.identity(X_cont)\n",
    "for feat, card in zip(cat_placeholders, cat_cardinalities):\n",
    "    X = embed_and_attach(X, feat, card)\n",
    "\n",
    "# Define the network layers. \n",
    "# Overfitting is a challenge so add L2 regularisation to weights in 1st layer & \n",
    "# a couple of dropout layers\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden_layer_1 = tf.layers.dense(inputs=X,\n",
    "                                     units=N_HIDDEN_1,\n",
    "                                     name='first_hidden_layer',\n",
    "                                     kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.3))\n",
    "    hidden_layer_1 = tf.layers.batch_normalization(hidden_layer_1, training=train_mode)\n",
    "    hidden_layer_1 = tf.nn.relu(hidden_layer_1)\n",
    "\n",
    "    drop_layer_1 = tf.layers.dropout(inputs=hidden_layer_1, \n",
    "                                     rate=0.4, \n",
    "                                     name='first_dropout_layer',\n",
    "                                     training=train_mode)\n",
    "\n",
    "    hidden_layer_2 = tf.layers.dense(inputs=drop_layer_1,\n",
    "                                     units=N_HIDDEN_2,\n",
    "                                     name='second_hidden_layer',\n",
    "                                     kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1))\n",
    "    hidden_layer_2 = tf.layers.batch_normalization(hidden_layer_2, training=train_mode)\n",
    "    hidden_layer_2 = tf.nn.relu(hidden_layer_2)\n",
    "                                     \n",
    "    drop_layer_2 = tf.layers.dropout(inputs=hidden_layer_2, \n",
    "                                     rate=0.2, \n",
    "                                     name='second_dropout_layer',\n",
    "                                     training=train_mode)\n",
    "\n",
    "    hidden_layer_3 = tf.layers.dense(inputs=drop_layer_2,\n",
    "                                     units=N_HIDDEN_3,\n",
    "                                     name='third_hidden_layer')\n",
    "    hidden_layer_3 = tf.layers.batch_normalization(hidden_layer_3, training=train_mode)\n",
    "    hidden_layer_3 = tf.nn.relu(hidden_layer_3)\n",
    "    \n",
    "    hidden_layer_4 = tf.layers.dense(inputs=hidden_layer_3,\n",
    "                                     units=N_HIDDEN_3,\n",
    "                                     name='fourth_hidden_layer')\n",
    "    hidden_layer_4 = tf.layers.batch_normalization(hidden_layer_4, training=train_mode)\n",
    "    hidden_layer_4 = tf.nn.relu(hidden_layer_4)\n",
    "\n",
    "    logits = tf.layers.dense(inputs=hidden_layer_4,\n",
    "                             units=n_classes,\n",
    "                             name='outputs')\n",
    "\n",
    "# Define the loss function for training as cross entropy\n",
    "with tf.name_scope('loss'):\n",
    "    xent = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xent, name='loss')\n",
    "\n",
    "# Define the optimiser\n",
    "with tf.name_scope('train'):\n",
    "    optimiser = tf.train.AdamOptimizer()  # AdagradOptimizer(learning_rate=LEARNING_RATE)\n",
    "    train_step = optimiser.minimize(loss)\n",
    "\n",
    "# Output the class probabilities to I can get the AUC\n",
    "with tf.name_scope('eval'):\n",
    "    predict = tf.argmax(logits, axis=1, name='class_predictions')\n",
    "    predict_proba = tf.nn.softmax(logits, name='probability_predictions')\n",
    "\n",
    "# Initialisation node and saver\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(cat_feats_idx, cat_placeholders, cont_feats_idx, batch_X, batch_y=None,\n",
    "                 training=False):\n",
    "    \"\"\" Return a feed dict for the graph including all the categorical features\n",
    "    to embed \"\"\"\n",
    "    \n",
    "    # Continuous X features and the labels if training run\n",
    "    feed_dict = {X_cont: batch_X[:, cont_feats_idx]}\n",
    "    if batch_y is not None:\n",
    "        feed_dict[y] = batch_y\n",
    "    \n",
    "    # Loop through the categorical features to provide values for the placeholders\n",
    "    for idx, tensor in zip(cat_feats_idx, cat_placeholders):\n",
    "        feed_dict[tensor] = batch_X[:, idx].reshape(-1, ).astype(int)\n",
    "        \n",
    "    # Training mode or not\n",
    "    feed_dict[train_mode] = training\n",
    "        \n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 400 iterations over 30 epochs with batchsize 250 ...\n",
      "Epoch = 0, Train AUC = 0.73619328, Valid AUC = 0.72566055\n",
      "Epoch = 1, Train AUC = 0.74818579, Valid AUC = 0.73445487\n",
      "Epoch = 2, Train AUC = 0.75400312, Valid AUC = 0.73864669\n",
      "Epoch = 3, Train AUC = 0.75752210, Valid AUC = 0.74348639\n",
      "Epoch = 4, Train AUC = 0.75969024, Valid AUC = 0.74502640\n",
      "Epoch = 5, Train AUC = 0.76406315, Valid AUC = 0.74647632\n",
      "Epoch = 6, Train AUC = 0.76602364, Valid AUC = 0.74748479\n",
      "Epoch = 7, Train AUC = 0.76749858, Valid AUC = 0.75023248\n",
      "Epoch = 8, Train AUC = 0.77163124, Valid AUC = 0.75298089\n",
      "Epoch = 9, Train AUC = 0.77244582, Valid AUC = 0.75138514\n",
      "Epoch = 10, Train AUC = 0.77326467, Valid AUC = 0.75082746\n",
      "Epoch = 11, Train AUC = 0.77685033, Valid AUC = 0.75335110\n",
      "Epoch = 12, Train AUC = 0.77669939, Valid AUC = 0.75318769\n",
      "Epoch = 13, Train AUC = 0.78044679, Valid AUC = 0.75490285\n",
      "Epoch = 14, Train AUC = 0.78119993, Valid AUC = 0.75608580\n",
      "Epoch = 15, Train AUC = 0.78323622, Valid AUC = 0.75524088\n",
      "Epoch = 16, Train AUC = 0.78591464, Valid AUC = 0.75696138\n",
      "Epoch = 17, Train AUC = 0.78685553, Valid AUC = 0.75615487\n",
      "Epoch = 18, Train AUC = 0.78746153, Valid AUC = 0.75732925\n",
      "Epoch = 19, Train AUC = 0.78928150, Valid AUC = 0.75736723\n",
      "Epoch = 20, Train AUC = 0.79040477, Valid AUC = 0.75926844\n",
      "Epoch = 21, Train AUC = 0.78857876, Valid AUC = 0.75576251\n",
      "Epoch = 22, Train AUC = 0.79389095, Valid AUC = 0.75703098\n",
      "Epoch = 23, Train AUC = 0.79359585, Valid AUC = 0.75852633\n",
      "Early stopping due to no improvement after 3 epochs.\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "train_auc, valid_auc = [], []\n",
    "n_rounds_not_improved = 0\n",
    "early_stopping_epochs = 3\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    init.run()\n",
    "\n",
    "    # Begin epoch loop\n",
    "    print('Training for {} iterations over {} epochs with batchsize {} ...'\n",
    "          .format(N_ITERATIONS, N_EPOCHS, BATCH_SIZE))\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        \n",
    "        # Iteration loop\n",
    "        for iteration in range(N_ITERATIONS):\n",
    "\n",
    "            # Get random selection of data for batch GD. Upsample positive classes to make it\n",
    "            # balanced in the training batch\n",
    "            pos_ratio = 0.5\n",
    "            pos_idx = np.random.choice(np.where(y_train[:, 1] == 1)[0], \n",
    "                                       size=int(np.round(BATCH_SIZE*pos_ratio)))\n",
    "            neg_idx = np.random.choice(np.where(y_train[:, 1] == 0)[0], \n",
    "                                       size=int(np.round(BATCH_SIZE*(1-pos_ratio))))\n",
    "            idx = np.concatenate([pos_idx, neg_idx])\n",
    "            \n",
    "            # Run training\n",
    "            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            sess.run([train_step, extra_update_ops], \n",
    "                     feed_dict=get_feed_dict(cat_feats_idx, cat_placeholders, cont_feats_idx, \n",
    "                                             X_train[idx, :], y_train[idx, :], 1))\n",
    "\n",
    "        # Check on the AUC\n",
    "        y_pred_train, y_prob_train = sess.run(\n",
    "            [predict, predict_proba], feed_dict=get_feed_dict(\n",
    "                cat_feats_idx, cat_placeholders, cont_feats_idx, X_train, y_train, False))\n",
    "        train_auc.append(roc_auc_score(y_train[:, 1], y_prob_train[:, 1]))\n",
    "        \n",
    "        y_pred_val, y_prob_val = sess.run(\n",
    "            [predict, predict_proba], feed_dict=get_feed_dict(\n",
    "                cat_feats_idx, cat_placeholders, cont_feats_idx, X_valid, y_valid, False))\n",
    "        valid_auc.append(roc_auc_score(y_valid[:, 1], y_prob_val[:, 1]))\n",
    "        \n",
    "        # Early stopping\n",
    "        if epoch > 1:\n",
    "            best_epoch_so_far = np.argmax(valid_auc[:-1])\n",
    "            if valid_auc[epoch] <= valid_auc[best_epoch_so_far]:\n",
    "                n_rounds_not_improved += 1\n",
    "            else:\n",
    "                n_rounds_not_improved = 0       \n",
    "            if n_rounds_not_improved > early_stopping_epochs:\n",
    "                print('Early stopping due to no improvement after {} epochs.'\n",
    "                      .format(early_stopping_epochs))\n",
    "                break\n",
    "        print('Epoch = {}, Train AUC = {:.8f}, Valid AUC = {:.8f}'\n",
    "              .format(epoch, train_auc[epoch], valid_auc[epoch]))\n",
    "\n",
    "    # Once trained, make predictions\n",
    "    print('Training complete.')\n",
    "    y_prob = sess.run(predict_proba, feed_dict=get_feed_dict(\n",
    "        cat_feats_idx, cat_placeholders, cont_feats_idx, predict_df, None, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.832731</td>\n",
       "      <td>0.167269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.228280</td>\n",
       "      <td>0.771720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.800431</td>\n",
       "      <td>0.199569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.871248</td>\n",
       "      <td>0.128752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.249577</td>\n",
       "      <td>0.750423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48739</td>\n",
       "      <td>0.646017</td>\n",
       "      <td>0.353983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48740</td>\n",
       "      <td>0.471324</td>\n",
       "      <td>0.528676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48741</td>\n",
       "      <td>0.813051</td>\n",
       "      <td>0.186949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48742</td>\n",
       "      <td>0.820381</td>\n",
       "      <td>0.179619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48743</td>\n",
       "      <td>0.204797</td>\n",
       "      <td>0.795202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48744 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1\n",
       "0      0.832731  0.167269\n",
       "1      0.228280  0.771720\n",
       "2      0.800431  0.199569\n",
       "3      0.871248  0.128752\n",
       "4      0.249577  0.750423\n",
       "...         ...       ...\n",
       "48739  0.646017  0.353983\n",
       "48740  0.471324  0.528676\n",
       "48741  0.813051  0.186949\n",
       "48742  0.820381  0.179619\n",
       "48743  0.204797  0.795202\n",
       "\n",
       "[48744 rows x 2 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame(y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
